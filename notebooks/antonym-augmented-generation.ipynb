{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-26T16:13:27.747086Z",
     "iopub.status.busy": "2025-01-26T16:13:27.746857Z",
     "iopub.status.idle": "2025-01-26T16:13:28.206314Z",
     "shell.execute_reply": "2025-01-26T16:13:28.205425Z",
     "shell.execute_reply.started": "2025-01-26T16:13:27.747065Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/antdataset/antonyms.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:13:28.207715Z",
     "iopub.status.busy": "2025-01-26T16:13:28.207381Z",
     "iopub.status.idle": "2025-01-26T16:13:28.243769Z",
     "shell.execute_reply": "2025-01-26T16:13:28.242885Z",
     "shell.execute_reply.started": "2025-01-26T16:13:28.207674Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>·àã·ã≠</td>\n",
       "      <td>·â≥·âΩ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>·àã·ã≠</td>\n",
       "      <td>·àã·àç·àò·àà·ä®·âµ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>·àã·ã≠</td>\n",
       "      <td>·àã·àã·ã≠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>·çç·àã·åé·â∂·âΩ</td>\n",
       "      <td>·å•·àã·âª·ãé·âΩ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>·åç·âµ·à≠</td>\n",
       "      <td>·ã®·àö·â∞·å£·å†·çç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>·â∞·å†·äì·ä®·à®</td>\n",
       "      <td>·â∞·ã≥·ä®·àò</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>·â∞·å†·äì·ä®·à®</td>\n",
       "      <td>·àã·àã</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>·àô·àâ ·â†·àô·àâ</td>\n",
       "      <td>·â†·ä®·çä·àç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>·â∞·à∞·äì·ä≠·àè·àç</td>\n",
       "      <td>·ä†·àç·çè·àç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>·â∞·à∞·äì·ä≠·àè·àç</td>\n",
       "      <td>·â∞·å†·äì·ä≠·àØ·àç</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word1   word2\n",
       "0      ·àã·ã≠      ·â≥·âΩ\n",
       "1      ·àã·ã≠  ·àã·àç·àò·àà·ä®·âµ\n",
       "2      ·àã·ã≠     ·àã·àã·ã≠\n",
       "3   ·çç·àã·åé·â∂·âΩ   ·å•·àã·âª·ãé·âΩ\n",
       "4     ·åç·âµ·à≠  ·ã®·àö·â∞·å£·å†·çç\n",
       "5   ·â∞·å†·äì·ä®·à®    ·â∞·ã≥·ä®·àò\n",
       "6   ·â∞·å†·äì·ä®·à®      ·àã·àã\n",
       "7  ·àô·àâ ·â†·àô·àâ    ·â†·ä®·çä·àç\n",
       "8  ·â∞·à∞·äì·ä≠·àè·àç    ·ä†·àç·çè·àç\n",
       "9  ·â∞·à∞·äì·ä≠·àè·àç  ·â∞·å†·äì·ä≠·àØ·àç"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fdataset (ensure your CSV is UTF-8 encoded)\n",
    "data = pd.read_csv('/kaggle/input/antdataset/antonyms.csv')\n",
    "data = data[['word1','word2']]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:14:20.027108Z",
     "iopub.status.busy": "2025-01-26T16:14:20.026778Z",
     "iopub.status.idle": "2025-01-26T16:14:20.043629Z",
     "shell.execute_reply": "2025-01-26T16:14:20.043000Z",
     "shell.execute_reply.started": "2025-01-26T16:14:20.027081Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word1   word2\n",
      "0     ·àã·ã≠      ·â≥·âΩ\n",
      "1     ·àã·ã≠  ·àã·àç·àò·àà·ä®·âµ\n",
      "2     ·àã·ã≠     ·àã·àã·ã≠\n",
      "3  ·çç·àã·åé·â∂·âΩ   ·å•·àã·âª·ãé·âΩ\n",
      "4    ·åç·âµ·à≠  ·ã®·àö·â∞·å£·å†·çç\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original dataset\n",
    "data = pd.read_csv('/kaggle/input/antdataset/antonyms.csv')\n",
    "data = data[['word1', 'word2']]\n",
    "\n",
    "# Create a swapped DataFrame\n",
    "swapped_data = data.rename(columns={'word1': 'word2', 'word2': 'word1'})\n",
    "\n",
    "# Combine the original and swapped DataFrames\n",
    "augmented_data = pd.concat([data, swapped_data]).reset_index(drop=True)\n",
    "\n",
    "# Save the augmented data to a new CSV file\n",
    "#augmented_data.to_csv('/kaggle/working/augmented_antonyms.csv', index=False)\n",
    "\n",
    "# Display the augmented data\n",
    "print(augmented_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:23:41.312109Z",
     "iopub.status.busy": "2025-01-26T15:23:41.311825Z",
     "iopub.status.idle": "2025-01-26T15:23:41.317466Z",
     "shell.execute_reply": "2025-01-26T15:23:41.316782Z",
     "shell.execute_reply.started": "2025-01-26T15:23:41.312090Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4766, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:14:24.829152Z",
     "iopub.status.busy": "2025-01-26T16:14:24.828869Z",
     "iopub.status.idle": "2025-01-26T16:14:25.346432Z",
     "shell.execute_reply": "2025-01-26T16:14:25.345599Z",
     "shell.execute_reply.started": "2025-01-26T16:14:24.829131Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test datasets saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset\n",
    "# file_path = \"/kaggle/input/antdataset/antonyms.csv\"\n",
    "# data = pd.read_csv(file_path)\n",
    "# data = data[['word1','word2']]\n",
    "\n",
    "# Split data into 80% train and 20% test\n",
    "train_data, test_data = train_test_split(augmented_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the splits\n",
    "train_data.to_csv(\"train.csv\", index=False)\n",
    "test_data.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(\"Train and test datasets saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:14:28.370429Z",
     "iopub.status.busy": "2025-01-26T16:14:28.370128Z",
     "iopub.status.idle": "2025-01-26T16:14:29.033360Z",
     "shell.execute_reply": "2025-01-26T16:14:29.032712Z",
     "shell.execute_reply.started": "2025-01-26T16:14:28.370406Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"xxxxxxxxxxxxxxxxxxxxxxx\")  # Replace with your actual token\n",
    "# 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\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:23:56.022196Z",
     "iopub.status.busy": "2025-01-26T15:23:56.021913Z",
     "iopub.status.idle": "2025-01-26T15:23:56.025524Z",
     "shell.execute_reply": "2025-01-26T15:23:56.024716Z",
     "shell.execute_reply.started": "2025-01-26T15:23:56.022174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#textdetox/mbart-detox-baseline\n",
    "#facebook/m2m100_418M\n",
    "#facebook/m2m100_1.2B\n",
    "#google/mt5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:14:33.448231Z",
     "iopub.status.busy": "2025-01-26T16:14:33.447944Z",
     "iopub.status.idle": "2025-01-26T16:14:34.521265Z",
     "shell.execute_reply": "2025-01-26T16:14:34.520569Z",
     "shell.execute_reply.started": "2025-01-26T16:14:33.448210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f0f71e5a7340d1a9a08c4385ef2d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1642a8c12f4dbb936cba62e1aada47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['word1', 'word2'],\n",
      "        num_rows: 3812\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['word1', 'word2'],\n",
      "        num_rows: 954\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the CSV files as datasets\n",
    "data_files = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# Check the loaded dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:14:38.826966Z",
     "iopub.status.busy": "2025-01-26T16:14:38.826503Z",
     "iopub.status.idle": "2025-01-26T16:15:02.261832Z",
     "shell.execute_reply": "2025-01-26T16:15:02.261122Z",
     "shell.execute_reply.started": "2025-01-26T16:14:38.826940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455f87104f854084b674d5244046be36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98771910266c4a738660a8a63592b514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe517ae84d3b461392346e8545c629de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/3.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02d0e23cbaa46dcb2cd9d7c7a21a5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b038c3d4e924234a2d02b1a88a0a882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bba8494ba14187b6e411f46a636729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7d78a2d3774c44839fae075ae91881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/954 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['word1', 'word2', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3812\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['word1', 'word2', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 954\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "# Load mT5 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\", )\n",
    "\n",
    "# Define tokenization function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"Antonym of: {str(input_text)}\" for input_text in examples[\"word1\"]]\n",
    "    targets = [str(target_text) for target_text in examples[\"word2\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=32, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=32, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Check tokenized dataset\n",
    "print(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T19:20:24.846515Z",
     "iopub.status.busy": "2025-01-14T19:20:24.846232Z",
     "iopub.status.idle": "2025-01-14T19:20:24.850955Z",
     "shell.execute_reply": "2025-01-14T19:20:24.850176Z",
     "shell.execute_reply.started": "2025-01-14T19:20:24.846494Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128022, 3968, 2], 'attention_mask': [1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "example_text = \"·àã·ã≠\"\n",
    "tokenized_example = tokenizer(example_text)\n",
    "print(tokenized_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T19:20:48.535170Z",
     "iopub.status.busy": "2025-01-14T19:20:48.534862Z",
     "iopub.status.idle": "2025-01-14T19:20:48.539602Z",
     "shell.execute_reply": "2025-01-14T19:20:48.538853Z",
     "shell.execute_reply.started": "2025-01-14T19:20:48.535147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__en__', '‚ñÅ·àã·ã≠', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens([128022, 3968, 2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T19:20:55.646452Z",
     "iopub.status.busy": "2025-01-14T19:20:55.646149Z",
     "iopub.status.idle": "2025-01-14T19:21:02.525783Z",
     "shell.execute_reply": "2025-01-14T19:21:02.525032Z",
     "shell.execute_reply.started": "2025-01-14T19:20:55.646429Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: {'input_ids': [128022, 3968, 3873, 11041, 5216, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "Decoded: ·àã·ã≠ ·ä•·äì ·â≥·âΩ\n"
     ]
    }
   ],
   "source": [
    "example_text = \"·àã·ã≠ ·ä•·äì ·â≥·âΩ\"\n",
    "tokenized_example = tokenizer(example_text)\n",
    "decoded_text = tokenizer.decode(tokenized_example[\"input_ids\"], skip_special_tokens=True)\n",
    "\n",
    "print(\"Tokenized:\", tokenized_example)\n",
    "print(\"Decoded:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T19:21:06.514680Z",
     "iopub.status.busy": "2025-01-14T19:21:06.514079Z",
     "iopub.status.idle": "2025-01-14T19:21:06.521522Z",
     "shell.execute_reply": "2025-01-14T19:21:06.520717Z",
     "shell.execute_reply.started": "2025-01-14T19:21:06.514653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['·çà·à™', ' ·â∞·àà·ãã·ãã·å≠·äê·âµ', ' ·àπ·àç', ' ·ãì·àò·â≥·ãä', ' ·ä†·å†·âÉ·àã·ã≠']\n"
     ]
    }
   ],
   "source": [
    "train_dataset=tokenized_dataset[\"train\"]\n",
    "print(train_dataset[\"word2\"][:5])  # Check if target texts are valid Amharic antonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:25:06.055709Z",
     "iopub.status.busy": "2025-01-26T15:25:06.055389Z",
     "iopub.status.idle": "2025-01-26T15:51:55.089126Z",
     "shell.execute_reply": "2025-01-26T15:51:55.088181Z",
     "shell.execute_reply.started": "2025-01-26T15:25:06.055683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d2e596ac4249428e5717878e108280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aed8cc434384b78bd7cbaf7b802a877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-10-c676684cbec1>:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomSeq2SeqTrainer(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1080/12000 25:47 < 4:21:11, 0.70 it/s, Epoch 9/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.301700</td>\n",
       "      <td>3.034365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.579400</td>\n",
       "      <td>0.544060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>0.386595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.369854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.294000</td>\n",
       "      <td>0.360923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.254800</td>\n",
       "      <td>0.358269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.240700</td>\n",
       "      <td>0.357911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.218000</td>\n",
       "      <td>0.361463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.360311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301aadb1319840cfa249c8e63d96ce61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1737905119.f9b8a834e73d.31.0:   0%|          | 0.00/31.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Beck90/m2m-amharic-antonym-augmented/commit/4ddc0e3b4dd2b27618f11a3dc68895fec1dd1530', commit_message='End of training', commit_description='', oid='4ddc0e3b4dd2b27618f11a3dc68895fec1dd1530', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Beck90/m2m-amharic-antonym-augmented', endpoint='https://huggingface.co', repo_type='model', repo_id='Beck90/m2m-amharic-antonym-augmented'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "# Load mT5 model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./m2m-amharic-antonym-augmented\",  # Directory to save the model\n",
    "    evaluation_strategy=\"epoch\",        # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,                 # Learning rate\n",
    "    per_device_train_batch_size=16,     # Training batch size\n",
    "    per_device_eval_batch_size=16,      # Evaluation batch size\n",
    "    num_train_epochs=100,                 # Number of epochs\n",
    "    weight_decay=0.01,                  # Weight decay for regularization\n",
    "    predict_with_generate=True,         # Allow prediction generation\n",
    "   # logging_dir=\"./logs\",               # Directory for logs\n",
    "    logging_steps=10,                   # Log every 10 steps\n",
    "    push_to_hub=True,                   # Enable pushing to Hugging Face hub\n",
    "    report_to=\"tensorboard\",\n",
    "    save_strategy= \"epoch\",  \n",
    "    save_total_limit=1,                 # Limit on the number of saved checkpoints\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "#from transformers import Seq2SeqTrainer\n",
    "\n",
    "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def _save(self, output_dir: str, state_dict=None):\n",
    "        if state_dict is None:\n",
    "            state_dict = self.model.state_dict()\n",
    "        \n",
    "        # Make all tensors in the state_dict contiguous\n",
    "        for key, tensor in state_dict.items():\n",
    "            if not tensor.is_contiguous():\n",
    "                state_dict[key] = tensor.contiguous()\n",
    "        \n",
    "        # Call the original save method\n",
    "        super()._save(output_dir, state_dict)\n",
    "\n",
    "trainer = CustomSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "# Push final model to Hugging Face Hub\n",
    "trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:53:16.858964Z",
     "iopub.status.busy": "2025-01-26T15:53:16.858681Z",
     "iopub.status.idle": "2025-01-26T15:53:20.621690Z",
     "shell.execute_reply": "2025-01-26T15:53:20.620941Z",
     "shell.execute_reply.started": "2025-01-26T15:53:16.858944Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "·àã·ã≠\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./m2m-amharic-antonym-augmented\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate an antonym\n",
    "output = antonym_generator(\"·àã·ã≠\")\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:54:25.525677Z",
     "iopub.status.busy": "2025-01-26T15:54:25.525293Z",
     "iopub.status.idle": "2025-01-26T15:55:15.292179Z",
     "shell.execute_reply": "2025-01-26T15:55:15.291295Z",
     "shell.execute_reply.started": "2025-01-26T15:54:25.525649Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1258161a73b46438b91e8b86574151d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf71e090acd4e21b2477bfc8a0a39df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2d28b098434338b3c2b2f99e468002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "·àã·ã≠\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-antonym-augmented\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline with GPU\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# Generate an antonym\n",
    "input_text = \"Antonym of: ·àã·ã≠\"\n",
    "output = antonym_generator(input_text, max_length=16, num_return_sequences=1)\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:55:29.531062Z",
     "iopub.status.busy": "2025-01-26T15:55:29.530772Z",
     "iopub.status.idle": "2025-01-26T15:56:40.157510Z",
     "shell.execute_reply": "2025-01-26T15:56:40.156553Z",
     "shell.execute_reply.started": "2025-01-26T15:55:29.531042Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.26%\n",
      "BLEU Score: 0.37314820307067903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "test_dataset=tokenized_dataset[\"test\"]\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "for word in test_dataset[\"word1\"]:\n",
    "    # Tokenize the input word and move it to the same device as the model\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Generate antonym and decode\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=5)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Compute accuracy\n",
    "true_antonyms = test_dataset[\"word2\"]  # Ground truth antonyms\n",
    "accuracy = accuracy_score(true_antonyms, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute BLEU score (optional, for sequence evaluation)\n",
    "references = [[ref.split()] for ref in true_antonyms]\n",
    "predictions_tokenized = [pred.split() for pred in predictions]\n",
    "bleu = corpus_bleu(references, predictions_tokenized)\n",
    "print(f\"BLEU Score: {bleu}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:58:20.538284Z",
     "iopub.status.busy": "2025-01-26T15:58:20.537969Z",
     "iopub.status.idle": "2025-01-26T15:59:30.008916Z",
     "shell.execute_reply": "2025-01-26T15:59:30.007935Z",
     "shell.execute_reply.started": "2025-01-26T15:58:20.538260Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.78%\n",
      "BLEU Score: 0.39607242290087397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "test_dataset = tokenized_dataset[\"test\"]\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "for word in test_dataset[\"word1\"]:\n",
    "    # Add the prompt to the input\n",
    "    input_text = f\"Antonym of: {word}\"\n",
    "    \n",
    "    # Tokenize the input word and move it to the same device as the model\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Generate antonym and decode\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=5)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Compute accuracy\n",
    "true_antonyms = test_dataset[\"word2\"]  # Ground truth antonyms\n",
    "accuracy = accuracy_score(true_antonyms, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute BLEU score (optional, for sequence evaluation)\n",
    "references = [[ref.split()] for ref in true_antonyms]\n",
    "predictions_tokenized = [pred.split() for pred in predictions]\n",
    "bleu = corpus_bleu(references, predictions_tokenized)\n",
    "print(f\"BLEU Score: {bleu}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:55:46.176751Z",
     "iopub.status.busy": "2025-01-26T16:55:46.176414Z",
     "iopub.status.idle": "2025-01-26T16:56:37.259610Z",
     "shell.execute_reply": "2025-01-26T16:56:37.258628Z",
     "shell.execute_reply.started": "2025-01-26T16:55:46.176715Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3017022829254ebf885d4927fff74c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cb9461307d4793991955d088323649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b67b61338ef412c8dc61ff888c5fec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ·â†·àã | Antonym: ·â†·àã\n",
      "Word: ·â•·à≠·ãµ | Antonym: ·àô·âÄ·âµ\n",
      "Word: ·â†·à®·ã∞ | Antonym: ·äê·ã∞·ã∞\n",
      "Word: ·à∞·å† | Antonym: ·å†·çã\n",
      "Word: ·ä†·ãà·à´ | Antonym: ·äê·å†·àã\n",
      "Word: ·àÑ·ã∞ | Antonym: ·äê·ã∞·ã∞\n",
      "Word: ·ã´·ãò | Antonym: ·ã´·äê·à∞\n",
      "Word: ·àò·àç·ä´·àù | Antonym: ·ä†·àµ·âÄ·ã´·àö\n",
      "Word: ·àò·å•·çé | Antonym: ·ã∞·àÖ·äì\n",
      "Word: ·ä†·àµ·âÄ·ã´·àö | Antonym: ·ä†·àµ·âÄ·ã´·àö\n",
      "Word: ·çç·àã·åé·âµ | Antonym: ·çç·àã·åé·âµ\n",
      "Word: ·àç·â£·àù | Antonym: ·ä•·å•·à®·âµ\n",
      "Word: ·ä®·çç | Antonym: ·ãù·âÖ\n",
      "Word: ·çç·âÖ·à≠ | Antonym: ·å†·â•·âÖ\n",
      "Word: ·àô·àâ | Antonym: ·â£·ã∂\n",
      "Word: ·âµ·àç·âÖ | Antonym: ·âµ·äï·àΩ\n",
      "Word: ·âÄ·àã·àç | Antonym: ·ä†·àµ·â∏·åã·à™\n",
      "Word: ·à∞·àã·àù | Antonym: ·ä†·àà·àò·åç·â£·â£·âµ\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-antonym-augmented\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# List of words for which to generate antonyms\n",
    "words = [\"·â†·àã\", \"·â•·à≠·ãµ\", \"·â†·à®·ã∞\", \"·à∞·å†\", \"·ä†·ãà·à´\", \"·àÑ·ã∞\", \"·ã´·ãò\", \"·àò·àç·ä´·àù\", \"·àò·å•·çé\", \"·ä†·àµ·âÄ·ã´·àö\", \n",
    "         \"·çç·àã·åé·âµ\", \"·àç·â£·àù\", \"·ä®·çç\", \"·çç·âÖ·à≠\", \"·àô·àâ\", \"·âµ·àç·âÖ\", \"·âÄ·àã·àç\", \"·à∞·àã·àù\"]\n",
    "\n",
    "# Generate antonyms for each word\n",
    "for word in words:\n",
    "    prompt = f\"Antonym of: {word}\"\n",
    "    output = antonym_generator(prompt)\n",
    "    print(f\"Word: {word} | Antonym: {output[0]['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T17:02:15.034915Z",
     "iopub.status.busy": "2025-01-26T17:02:15.034586Z",
     "iopub.status.idle": "2025-01-26T17:03:06.161507Z",
     "shell.execute_reply": "2025-01-26T17:03:06.160680Z",
     "shell.execute_reply.started": "2025-01-26T17:02:15.034890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcba24fdf314622b449666a4af09dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/931 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245d466f5c8f4f0198470b40323f7466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14518df3a504845ad95838d88a0a666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ·â†·àã | Antonym: ·ä†·àÆ·åå\n",
      "Word: ·â•·à≠·ãµ | Antonym: ·ä†·àà·àò·åç·â£·â£·âµ\n",
      "Word: ·â†·à®·ã∞ | Antonym: ·ã´·àç·â∞·àà·àò·ã∞\n",
      "Word: ·à∞·å† | Antonym: ·â∞·ä®·çà·â∞\n",
      "Word: ·ä†·ãà·à´ | Antonym: ·â∞·å®·àõ·à™\n",
      "Word: ·àÑ·ã∞ | Antonym: ·â∞·ä®·çà·â∞\n",
      "Word: ·ã´·ãò | Antonym: ·ã´·à®·åã·åã·àç\n",
      "Word: ·àò·àç·ä´·àù | Antonym: ·å®·ä´·äù\n",
      "Word: ·àò·å•·çé | Antonym: ·â∞·àµ·àõ·àö\n",
      "Word: ·ä†·àµ·âÄ·ã´·àö | Antonym: ·ä†·àµ·ã∞·à≥·âΩ\n",
      "Word: ·çç·àã·åé·âµ | Antonym: ·çç·àã·åé·âµ\n",
      "Word: ·àç·â£·àù | Antonym: ·àç·àõ·ãµ\n",
      "Word: ·ä®·çç | Antonym: ·ãù·âÖ·â∞·äõ\n",
      "Word: ·çç·âÖ·à≠ | Antonym: ·å•·àã·âª\n",
      "Word: ·àô·àâ | Antonym: ·âÄ·å≠·äï\n",
      "Word: ·âµ·àç·âÖ | Antonym: ·å•·âÇ·âµ\n",
      "Word: ·âÄ·àã·àç | Antonym: ·ã´·àç·â∞·àà·àò·ã∞\n",
      "Word: ·à∞·àã·àù | Antonym: ·ä†·àà·àò·åç·â£·â£·âµ\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-antonym\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# List of words for which to generate antonyms\n",
    "words = [\"·â†·àã\", \"·â•·à≠·ãµ\", \"·â†·à®·ã∞\", \"·à∞·å†\", \"·ä†·ãà·à´\", \"·àÑ·ã∞\", \"·ã´·ãò\", \"·àò·àç·ä´·àù\", \"·àò·å•·çé\", \"·ä†·àµ·âÄ·ã´·àö\", \n",
    "         \"·çç·àã·åé·âµ\", \"·àç·â£·àù\", \"·ä®·çç\", \"·çç·âÖ·à≠\", \"·àô·àâ\", \"·âµ·àç·âÖ\", \"·âÄ·àã·àç\", \"·à∞·àã·àù\"]\n",
    "\n",
    "# Generate antonyms for each word\n",
    "for word in words:\n",
    "    prompt = f\"Antonym of: {word}\"\n",
    "    output = antonym_generator(prompt)\n",
    "    print(f\"Word: {word} | Antonym: {output[0]['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T17:06:22.178479Z",
     "iopub.status.busy": "2025-01-26T17:06:22.178151Z",
     "iopub.status.idle": "2025-01-26T17:07:12.883373Z",
     "shell.execute_reply": "2025-01-26T17:07:12.882523Z",
     "shell.execute_reply.started": "2025-01-26T17:06:22.178456Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534a372053da4cd193bfce84b977d1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/931 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa33fb2e8b3413ca7fa5cfa0732ec45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9006a1305b4e419384d7d82a49d1edff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ·â†·àã | Antonym: ·ä†·âã·à≠·å•\n",
      "Word: ·â•·à≠·ãµ | Antonym: ·ã∞·çã·à≠\n",
      "Word: ·â†·à®·ã∞ | Antonym: ·ã´·àç·â∞·àà·àò·ã∞\n",
      "Word: ·à∞·å† | Antonym: ·å†·çã\n",
      "Word: ·ä†·ãà·à´ | Antonym: ·çà·à≥·àΩ\n",
      "Word: ·àÑ·ã∞ | Antonym: ·â∞·ã∞·à∞·â∞\n",
      "Word: ·ã´·ãò | Antonym: ·ã´·à®·åã·åã·àç\n",
      "Word: ·àò·àç·ä´·àù | Antonym: ·å•·äï·ä´·à¨\n",
      "Word: ·àò·å•·çé | Antonym: ·â∞·å£·â•·âã·àç\n",
      "Word: ·ä†·àµ·âÄ·ã´·àö | Antonym: ·ä†·àµ·âÄ·ã´·àö\n",
      "Word: ·çç·àã·åé·âµ | Antonym: ·çç·àã·åé·âµ\n",
      "Word: ·àç·â£·àù | Antonym: ·àç·â£·àù\n",
      "Word: ·ä®·çç | Antonym: ·ä®·çç·â∞·äõ\n",
      "Word: ·çç·âÖ·à≠ | Antonym: ·å•·àã·âª\n",
      "Word: ·àô·àâ | Antonym: ·â†·ä®·çä·àç\n",
      "Word: ·âµ·àç·âÖ | Antonym: ·âµ·àç·âÖ\n",
      "Word: ·âÄ·àã·àç | Antonym: ·âÄ·àã·àç\n",
      "Word: ·à∞·àã·àù | Antonym: ·ä†·àà·àò·åç·â£·â£·âµ\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-antonym2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# List of words for which to generate antonyms\n",
    "words = [\"·â†·àã\", \"·â•·à≠·ãµ\", \"·â†·à®·ã∞\", \"·à∞·å†\", \"·ä†·ãà·à´\", \"·àÑ·ã∞\", \"·ã´·ãò\", \"·àò·àç·ä´·àù\", \"·àò·å•·çé\", \"·ä†·àµ·âÄ·ã´·àö\", \n",
    "         \"·çç·àã·åé·âµ\", \"·àç·â£·àù\", \"·ä®·çç\", \"·çç·âÖ·à≠\", \"·àô·àâ\", \"·âµ·àç·âÖ\", \"·âÄ·àã·àç\", \"·à∞·àã·àù\"]\n",
    "\n",
    "# Generate antonyms for each word\n",
    "for word in words:\n",
    "    prompt = f\"Antonym of: {word}\"\n",
    "    output = antonym_generator(prompt)\n",
    "    print(f\"Word: {word} | Antonym: {output[0]['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T17:11:30.398383Z",
     "iopub.status.busy": "2025-01-26T17:11:30.398053Z",
     "iopub.status.idle": "2025-01-26T17:12:21.581591Z",
     "shell.execute_reply": "2025-01-26T17:12:21.580776Z",
     "shell.execute_reply.started": "2025-01-26T17:11:30.398358Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c193098331d4dd19f94c356af468a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680d4729e8304aa0931b945ac3aa1602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83733ddde54a4a7aa69d98e5f7f31c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ·â†·àã | Antonym: ·â†·àã\n",
      "Word: ·â•·à≠·ãµ | Antonym: ·â•·àç·àÖ·äê·âµ\n",
      "Word: ·â†·à®·ã∞ | Antonym: ·ã®·â∞·â†·àã·à∏\n",
      "Word: ·à∞·å† | Antonym: ·â∞·äï·à≥·çã·çä\n",
      "Word: ·ä†·ãà·à´ | Antonym: ·â∏·àç·â∞·äù·äê·âµ\n",
      "Word: ·àÑ·ã∞ | Antonym: ·â∞·äï·à≥·çã·çä\n",
      "Word: ·ã´·ãò | Antonym: ·â∏·àç·â∞·äù·äê·âµ\n",
      "Word: ·àò·àç·ä´·àù | Antonym: ·ä•·à®·çç·âµ\n",
      "Word: ·àò·å•·çé | Antonym: ·ã®·â∞·ãò·â†·à´·à®·âÄ\n",
      "Word: ·ä†·àµ·âÄ·ã´·àö | Antonym: ·âÄ·äì·â∞·äõ\n",
      "Word: ·çç·àã·åé·âµ | Antonym: ·çç·àã·åé·âµ\n",
      "Word: ·àç·â£·àù | Antonym: ·â•·àç·àÖ·äê·âµ\n",
      "Word: ·ä®·çç | Antonym: ·ä®·çç·â∞·äõ\n",
      "Word: ·çç·âÖ·à≠ | Antonym: ·ä•·â•·ãµ\n",
      "Word: ·àô·àâ | Antonym: ·à∞·çä\n",
      "Word: ·âµ·àç·âÖ | Antonym: ·âµ·àç·âÖ\n",
      "Word: ·âÄ·àã·àç | Antonym: ·âÄ·àã·àç\n",
      "Word: ·à∞·àã·àù | Antonym: ·ã≠·ã∞·à∞·â±\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-synonym\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# List of words for which to generate antonyms\n",
    "words = [\"·â†·àã\", \"·â•·à≠·ãµ\", \"·â†·à®·ã∞\", \"·à∞·å†\", \"·ä†·ãà·à´\", \"·àÑ·ã∞\", \"·ã´·ãò\", \"·àò·àç·ä´·àù\", \"·àò·å•·çé\", \"·ä†·àµ·âÄ·ã´·àö\", \n",
    "         \"·çç·àã·åé·âµ\", \"·àç·â£·àù\", \"·ä®·çç\", \"·çç·âÖ·à≠\", \"·àô·àâ\", \"·âµ·àç·âÖ\", \"·âÄ·àã·àç\", \"·à∞·àã·àù\"]\n",
    "\n",
    "# Generate antonyms for each word\n",
    "for word in words:\n",
    "    prompt = f\"Synonym of: {word}\"\n",
    "    output = antonym_generator(prompt)\n",
    "    print(f\"Word: {word} | Synonym: {output[0]['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T17:12:43.366765Z",
     "iopub.status.busy": "2025-01-26T17:12:43.366443Z",
     "iopub.status.idle": "2025-01-26T17:13:34.201440Z",
     "shell.execute_reply": "2025-01-26T17:13:34.200578Z",
     "shell.execute_reply.started": "2025-01-26T17:12:43.366736Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbf046974254d16a0a14b463863f666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b97d295e4d346fa87210927e65a1076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ffc5725bde4d9c92c699da72250e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ·â†·àã | Synonym: ·â†·àã\n",
      "Word: ·â•·à≠·ãµ | Synonym: ·â•·àç·àÖ·äê·âµ\n",
      "Word: ·â†·à®·ã∞ | Synonym: ·â∞·äï·à∏·à´·â≥·âΩ\n",
      "Word: ·à∞·å† | Synonym: ·à∞·â†·à∞·â†\n",
      "Word: ·ä†·ãà·à´ | Synonym: ·â∞·äï·à≥·çã·çä\n",
      "Word: ·àÑ·ã∞ | Synonym: ·â∞·äï·à∏·à´·â≥·âΩ\n",
      "Word: ·ã´·ãò | Synonym: ·â∞·äï·à∏·à´·â≥·âΩ\n",
      "Word: ·àò·àç·ä´·àù | Synonym: ·ä•·à®·çç·âµ\n",
      "Word: ·àò·å•·çé | Synonym: ·ã®·â∞·ãò·â†·à´·à®·âÄ\n",
      "Word: ·ä†·àµ·âÄ·ã´·àö | Synonym: ·ä†·àµ·ã∞·äï·åã·å≠\n",
      "Word: ·çç·àã·åé·âµ | Synonym: ·çç·àã·åé·âµ\n",
      "Word: ·àç·â£·àù | Synonym: ·â•·àç·àÖ·äê·âµ\n",
      "Word: ·ä®·çç | Synonym: ·ä®·çç·â∞·äõ\n",
      "Word: ·çç·âÖ·à≠ | Synonym: ·ä•·â•·ãµ\n",
      "Word: ·àô·àâ | Synonym: ·à∞·çä\n",
      "Word: ·âµ·àç·âÖ | Synonym: ·âµ·àç·âÖ\n",
      "Word: ·âÄ·àã·àç | Synonym: ·âÄ·àã·àç\n",
      "Word: ·à∞·àã·àù | Synonym: ·ã≠·ã∞·à∞·â±\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-synonym2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# List of words for which to generate antonyms\n",
    "words = [\"·â†·àã\", \"·â•·à≠·ãµ\", \"·â†·à®·ã∞\", \"·à∞·å†\", \"·ä†·ãà·à´\", \"·àÑ·ã∞\", \"·ã´·ãò\", \"·àò·àç·ä´·àù\", \"·àò·å•·çé\", \"·ä†·àµ·âÄ·ã´·àö\", \n",
    "         \"·çç·àã·åé·âµ\", \"·àç·â£·àù\", \"·ä®·çç\", \"·çç·âÖ·à≠\", \"·àô·àâ\", \"·âµ·àç·âÖ\", \"·âÄ·àã·àç\", \"·à∞·àã·àù\"]\n",
    "\n",
    "# Generate antonyms for each word\n",
    "for word in words:\n",
    "    prompt = f\"Synonym of: {word}\"\n",
    "    output = antonym_generator(prompt)\n",
    "    print(f\"Word: {word} | Synonym: {output[0]['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:15:02.263341Z",
     "iopub.status.busy": "2025-01-26T16:15:02.263019Z",
     "iopub.status.idle": "2025-01-26T16:39:13.473179Z",
     "shell.execute_reply": "2025-01-26T16:39:13.472234Z",
     "shell.execute_reply.started": "2025-01-26T16:15:02.263308Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c006e9101f4f208c5caa8faa477906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e477755717647d0b1faf922d3269dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-8-c8db64dfb8c7>:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomSeq2SeqTrainer(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='960' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  960/12000 23:11 < 4:27:18, 0.69 it/s, Epoch 8/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.297700</td>\n",
       "      <td>3.032696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.574600</td>\n",
       "      <td>0.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.368600</td>\n",
       "      <td>0.384104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.297200</td>\n",
       "      <td>0.367217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.293200</td>\n",
       "      <td>0.359345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.251600</td>\n",
       "      <td>0.355499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.240100</td>\n",
       "      <td>0.356763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.211700</td>\n",
       "      <td>0.357865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9086a5f44af64b069748bf008fd6dc68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1737908115.60566b86f3b3.31.0:   0%|          | 0.00/28.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Beck90/m2m-amharic-antonym2-augmented/commit/d2bce0a11dc09281e37b52359683b92026857fad', commit_message='End of training', commit_description='', oid='d2bce0a11dc09281e37b52359683b92026857fad', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Beck90/m2m-amharic-antonym2-augmented', endpoint='https://huggingface.co', repo_type='model', repo_id='Beck90/m2m-amharic-antonym2-augmented'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "# Load mT5 model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./m2m-amharic-antonym2-augmented\",  # Directory to save the model\n",
    "    evaluation_strategy=\"epoch\",        # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,                 # Learning rate\n",
    "    per_device_train_batch_size=16,     # Training batch size\n",
    "    per_device_eval_batch_size=16,      # Evaluation batch size\n",
    "    num_train_epochs=100,                 # Number of epochs\n",
    "    weight_decay=0.01,                  # Weight decay for regularization\n",
    "    predict_with_generate=True,         # Allow prediction generation\n",
    "   # logging_dir=\"./logs\",               # Directory for logs\n",
    "    logging_steps=10,                   # Log every 10 steps\n",
    "    push_to_hub=True,                   # Enable pushing to Hugging Face hub\n",
    "    report_to=\"tensorboard\",\n",
    "    save_strategy= \"epoch\",  \n",
    "    save_total_limit=1,                 # Limit on the number of saved checkpoints\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "#from transformers import Seq2SeqTrainer\n",
    "\n",
    "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def custom_loss(output_logits, target_ids, embeddings, alpha=0.1):\n",
    "        # Reshape logits and target_ids\n",
    "        logits = output_logits.view(-1, output_logits.size(-1))  # Flatten logits\n",
    "        targets = target_ids.view(-1)                           # Flatten target IDs\n",
    "    \n",
    "        # Cross-Entropy Loss with ignored padding tokens\n",
    "        ce_loss = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)(logits, targets)\n",
    "    \n",
    "        # Cosine Similarity Loss (penalize dissimilar embeddings)\n",
    "        similarity_loss = 1 - F.cosine_similarity(embeddings[:, 0, :], embeddings[:, 1, :]).mean()\n",
    "    \n",
    "        # Combine losses\n",
    "        return ce_loss + alpha * similarity_loss\n",
    "\n",
    "\n",
    "\n",
    "trainer = CustomSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "# Push final model to Hugging Face Hub\n",
    "trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:41:09.337221Z",
     "iopub.status.busy": "2025-01-26T16:41:09.336886Z",
     "iopub.status.idle": "2025-01-26T16:41:13.734533Z",
     "shell.execute_reply": "2025-01-26T16:41:13.733774Z",
     "shell.execute_reply.started": "2025-01-26T16:41:09.337194Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "·àã·ã≠\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./m2m-amharic-antonym2-augmented\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate an antonym\n",
    "output = antonym_generator(\"Antonym of: ·àã·ã≠\")\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:46:24.840061Z",
     "iopub.status.busy": "2025-01-26T16:46:24.839723Z",
     "iopub.status.idle": "2025-01-26T16:46:24.962078Z",
     "shell.execute_reply": "2025-01-26T16:46:24.961205Z",
     "shell.execute_reply.started": "2025-01-26T16:46:24.840036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "·àô·âÄ·âµ\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"./m2m-amharic-antonym2-augmented\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate an antonym\n",
    "output = antonym_generator(\"Antonym of: ·â•·à≠·ãµ\")\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:54:07.553650Z",
     "iopub.status.busy": "2025-01-26T16:54:07.553349Z",
     "iopub.status.idle": "2025-01-26T16:54:09.367535Z",
     "shell.execute_reply": "2025-01-26T16:54:09.366557Z",
     "shell.execute_reply.started": "2025-01-26T16:54:07.553626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ·â†·àã | Antonym: ·â†·ä®·àà\n",
      "Word: ·â•·à≠·ãµ | Antonym: ·àô·âÄ·âµ\n",
      "Word: ·â†·à®·ã∞ | Antonym: ·äê·ã∞·ã∞\n",
      "Word: ·à∞·å† | Antonym: ·äê·å†·àã\n",
      "Word: ·ä†·ãà·à´ | Antonym: ·ãç·à∞·ã±\n",
      "Word: ·àÑ·ã∞ | Antonym: ·â∞·ã∞·à∞·â∞\n",
      "Word: ·ã´·ãò | Antonym: ·ã´·äê·à∞\n",
      "Word: ·àò·àç·ä´·àù | Antonym: ·ä†·àµ·âÄ·ãµ·àû\n",
      "Word: ·àò·å•·çé | Antonym: ·ã∞·àÖ·äì\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ·ä†·àµ·âÄ·ã´·àö | Antonym: ·ä†·àµ·âÄ·ã´·àö\n",
      "Word: ·çç·àã·åé·âµ | Antonym: ·åâ·åâ·âµ\n",
      "Word: ·àç·â£·àù | Antonym: ·å†·â•·âÖ\n",
      "Word: ·ä®·çç | Antonym: ·ãù·âÖ\n",
      "Word: ·çç·âÖ·à≠ | Antonym: ·å†·â•·âÖ\n",
      "Word: ·àô·àâ | Antonym: ·â£·ã∂\n",
      "Word: ·âµ·àç·âÖ | Antonym: ·âµ·äï·àΩ\n",
      "Word: ·âÄ·àã·àç | Antonym: ·ä®·â£·ãµ\n",
      "Word: ·à∞·àã·àù | Antonym: ·â•·àç·àπ·äê·âµ\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"./m2m-amharic-antonym2-augmented\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# List of words for which to generate antonyms\n",
    "words = [\"·â†·àã\", \"·â•·à≠·ãµ\", \"·â†·à®·ã∞\", \"·à∞·å†\", \"·ä†·ãà·à´\", \"·àÑ·ã∞\", \"·ã´·ãò\", \"·àò·àç·ä´·àù\", \"·àò·å•·çé\", \"·ä†·àµ·âÄ·ã´·àö\", \n",
    "         \"·çç·àã·åé·âµ\", \"·àç·â£·àù\", \"·ä®·çç\", \"·çç·âÖ·à≠\", \"·àô·àâ\", \"·âµ·àç·âÖ\", \"·âÄ·àã·àç\", \"·à∞·àã·àù\"]\n",
    "\n",
    "# Generate antonyms for each word\n",
    "for word in words:\n",
    "    prompt = f\"Antonym of: {word}\"\n",
    "    output = antonym_generator(prompt)\n",
    "    print(f\"Word: {word} | Antonym: {output[0]['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:41:33.729205Z",
     "iopub.status.busy": "2025-01-26T16:41:33.728866Z",
     "iopub.status.idle": "2025-01-26T16:42:43.900453Z",
     "shell.execute_reply": "2025-01-26T16:42:43.899466Z",
     "shell.execute_reply.started": "2025-01-26T16:41:33.729175Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.15%\n",
      "BLEU Score: 0.3687150775982432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "test_dataset=tokenized_dataset[\"test\"]\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "for word in test_dataset[\"word1\"]:\n",
    "    # Tokenize the input word and move it to the same device as the model\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Generate antonym and decode\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=5)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Compute accuracy\n",
    "true_antonyms = test_dataset[\"word2\"]  # Ground truth antonyms\n",
    "accuracy = accuracy_score(true_antonyms, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute BLEU score (optional, for sequence evaluation)\n",
    "references = [[ref.split()] for ref in true_antonyms]\n",
    "predictions_tokenized = [pred.split() for pred in predictions]\n",
    "bleu = corpus_bleu(references, predictions_tokenized)\n",
    "print(f\"BLEU Score: {bleu}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:42:43.901721Z",
     "iopub.status.busy": "2025-01-26T16:42:43.901425Z",
     "iopub.status.idle": "2025-01-26T16:43:53.211040Z",
     "shell.execute_reply": "2025-01-26T16:43:53.210204Z",
     "shell.execute_reply.started": "2025-01-26T16:42:43.901676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.15%\n",
      "BLEU Score: 0.3656718058711154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "test_dataset = tokenized_dataset[\"test\"]\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "for word in test_dataset[\"word1\"]:\n",
    "    # Add the prompt to the input\n",
    "    input_text = f\"Antonym of: {word}\"\n",
    "    \n",
    "    # Tokenize the input word and move it to the same device as the model\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Generate antonym and decode\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=5)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Compute accuracy\n",
    "true_antonyms = test_dataset[\"word2\"]  # Ground truth antonyms\n",
    "accuracy = accuracy_score(true_antonyms, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute BLEU score (optional, for sequence evaluation)\n",
    "references = [[ref.split()] for ref in true_antonyms]\n",
    "predictions_tokenized = [pred.split() for pred in predictions]\n",
    "bleu = corpus_bleu(references, predictions_tokenized)\n",
    "print(f\"BLEU Score: {bleu}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6476400,
     "sourceId": 10461069,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6483494,
     "sourceId": 10471135,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
