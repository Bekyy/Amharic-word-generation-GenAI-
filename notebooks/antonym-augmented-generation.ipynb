{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-26T16:13:27.747086Z",
     "iopub.status.busy": "2025-01-26T16:13:27.746857Z",
     "iopub.status.idle": "2025-01-26T16:13:28.206314Z",
     "shell.execute_reply": "2025-01-26T16:13:28.205425Z",
     "shell.execute_reply.started": "2025-01-26T16:13:27.747065Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/antdataset/antonyms.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:13:28.207715Z",
     "iopub.status.busy": "2025-01-26T16:13:28.207381Z",
     "iopub.status.idle": "2025-01-26T16:13:28.243769Z",
     "shell.execute_reply": "2025-01-26T16:13:28.242885Z",
     "shell.execute_reply.started": "2025-01-26T16:13:28.207674Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ላይ</td>\n",
       "      <td>ታች</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ላይ</td>\n",
       "      <td>ላልመለከት</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ላይ</td>\n",
       "      <td>ላላይ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ፍላጎቶች</td>\n",
       "      <td>ጥላቻዎች</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ግትር</td>\n",
       "      <td>የሚተጣጠፍ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ተጠናከረ</td>\n",
       "      <td>ተዳከመ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ተጠናከረ</td>\n",
       "      <td>ላላ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ሙሉ በሙሉ</td>\n",
       "      <td>በከፊል</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ተሰናክሏል</td>\n",
       "      <td>አልፏል</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ተሰናክሏል</td>\n",
       "      <td>ተጠናክሯል</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word1   word2\n",
       "0      ላይ      ታች\n",
       "1      ላይ  ላልመለከት\n",
       "2      ላይ     ላላይ\n",
       "3   ፍላጎቶች   ጥላቻዎች\n",
       "4     ግትር  የሚተጣጠፍ\n",
       "5   ተጠናከረ    ተዳከመ\n",
       "6   ተጠናከረ      ላላ\n",
       "7  ሙሉ በሙሉ    በከፊል\n",
       "8  ተሰናክሏል    አልፏል\n",
       "9  ተሰናክሏል  ተጠናክሯል"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fdataset (ensure your CSV is UTF-8 encoded)\n",
    "data = pd.read_csv('/kaggle/input/antdataset/antonyms.csv')\n",
    "data = data[['word1','word2']]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:14:20.027108Z",
     "iopub.status.busy": "2025-01-26T16:14:20.026778Z",
     "iopub.status.idle": "2025-01-26T16:14:20.043629Z",
     "shell.execute_reply": "2025-01-26T16:14:20.043000Z",
     "shell.execute_reply.started": "2025-01-26T16:14:20.027081Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word1   word2\n",
      "0     ላይ      ታች\n",
      "1     ላይ  ላልመለከት\n",
      "2     ላይ     ላላይ\n",
      "3  ፍላጎቶች   ጥላቻዎች\n",
      "4    ግትር  የሚተጣጠፍ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original dataset\n",
    "data = pd.read_csv('/kaggle/input/antdataset/antonyms.csv')\n",
    "data = data[['word1', 'word2']]\n",
    "\n",
    "# Create a swapped DataFrame\n",
    "swapped_data = data.rename(columns={'word1': 'word2', 'word2': 'word1'})\n",
    "\n",
    "# Combine the original and swapped DataFrames\n",
    "augmented_data = pd.concat([data, swapped_data]).reset_index(drop=True)\n",
    "\n",
    "# Save the augmented data to a new CSV file\n",
    "#augmented_data.to_csv('/kaggle/working/augmented_antonyms.csv', index=False)\n",
    "\n",
    "# Display the augmented data\n",
    "print(augmented_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:23:41.312109Z",
     "iopub.status.busy": "2025-01-26T15:23:41.311825Z",
     "iopub.status.idle": "2025-01-26T15:23:41.317466Z",
     "shell.execute_reply": "2025-01-26T15:23:41.316782Z",
     "shell.execute_reply.started": "2025-01-26T15:23:41.312090Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4766, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:14:24.829152Z",
     "iopub.status.busy": "2025-01-26T16:14:24.828869Z",
     "iopub.status.idle": "2025-01-26T16:14:25.346432Z",
     "shell.execute_reply": "2025-01-26T16:14:25.345599Z",
     "shell.execute_reply.started": "2025-01-26T16:14:24.829131Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test datasets saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset\n",
    "# file_path = \"/kaggle/input/antdataset/antonyms.csv\"\n",
    "# data = pd.read_csv(file_path)\n",
    "# data = data[['word1','word2']]\n",
    "\n",
    "# Split data into 80% train and 20% test\n",
    "train_data, test_data = train_test_split(augmented_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the splits\n",
    "train_data.to_csv(\"train.csv\", index=False)\n",
    "test_data.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(\"Train and test datasets saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:14:28.370429Z",
     "iopub.status.busy": "2025-01-26T16:14:28.370128Z",
     "iopub.status.idle": "2025-01-26T16:14:29.033360Z",
     "shell.execute_reply": "2025-01-26T16:14:29.032712Z",
     "shell.execute_reply.started": "2025-01-26T16:14:28.370406Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"xxxxxxxxxxxxxxxxxxxxxxx\")  # Replace with your actual token\n",
    "# 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\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:23:56.022196Z",
     "iopub.status.busy": "2025-01-26T15:23:56.021913Z",
     "iopub.status.idle": "2025-01-26T15:23:56.025524Z",
     "shell.execute_reply": "2025-01-26T15:23:56.024716Z",
     "shell.execute_reply.started": "2025-01-26T15:23:56.022174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#textdetox/mbart-detox-baseline\n",
    "#facebook/m2m100_418M\n",
    "#facebook/m2m100_1.2B\n",
    "#google/mt5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:14:33.448231Z",
     "iopub.status.busy": "2025-01-26T16:14:33.447944Z",
     "iopub.status.idle": "2025-01-26T16:14:34.521265Z",
     "shell.execute_reply": "2025-01-26T16:14:34.520569Z",
     "shell.execute_reply.started": "2025-01-26T16:14:33.448210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f0f71e5a7340d1a9a08c4385ef2d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1642a8c12f4dbb936cba62e1aada47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['word1', 'word2'],\n",
      "        num_rows: 3812\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['word1', 'word2'],\n",
      "        num_rows: 954\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the CSV files as datasets\n",
    "data_files = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# Check the loaded dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:14:38.826966Z",
     "iopub.status.busy": "2025-01-26T16:14:38.826503Z",
     "iopub.status.idle": "2025-01-26T16:15:02.261832Z",
     "shell.execute_reply": "2025-01-26T16:15:02.261122Z",
     "shell.execute_reply.started": "2025-01-26T16:14:38.826940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455f87104f854084b674d5244046be36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98771910266c4a738660a8a63592b514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe517ae84d3b461392346e8545c629de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/3.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02d0e23cbaa46dcb2cd9d7c7a21a5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b038c3d4e924234a2d02b1a88a0a882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bba8494ba14187b6e411f46a636729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7d78a2d3774c44839fae075ae91881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/954 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['word1', 'word2', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3812\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['word1', 'word2', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 954\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "# Load mT5 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\", )\n",
    "\n",
    "# Define tokenization function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"Antonym of: {str(input_text)}\" for input_text in examples[\"word1\"]]\n",
    "    targets = [str(target_text) for target_text in examples[\"word2\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=32, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=32, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Check tokenized dataset\n",
    "print(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T19:20:24.846515Z",
     "iopub.status.busy": "2025-01-14T19:20:24.846232Z",
     "iopub.status.idle": "2025-01-14T19:20:24.850955Z",
     "shell.execute_reply": "2025-01-14T19:20:24.850176Z",
     "shell.execute_reply.started": "2025-01-14T19:20:24.846494Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128022, 3968, 2], 'attention_mask': [1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "example_text = \"ላይ\"\n",
    "tokenized_example = tokenizer(example_text)\n",
    "print(tokenized_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T19:20:48.535170Z",
     "iopub.status.busy": "2025-01-14T19:20:48.534862Z",
     "iopub.status.idle": "2025-01-14T19:20:48.539602Z",
     "shell.execute_reply": "2025-01-14T19:20:48.538853Z",
     "shell.execute_reply.started": "2025-01-14T19:20:48.535147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__en__', '▁ላይ', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens([128022, 3968, 2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T19:20:55.646452Z",
     "iopub.status.busy": "2025-01-14T19:20:55.646149Z",
     "iopub.status.idle": "2025-01-14T19:21:02.525783Z",
     "shell.execute_reply": "2025-01-14T19:21:02.525032Z",
     "shell.execute_reply.started": "2025-01-14T19:20:55.646429Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: {'input_ids': [128022, 3968, 3873, 11041, 5216, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "Decoded: ላይ እና ታች\n"
     ]
    }
   ],
   "source": [
    "example_text = \"ላይ እና ታች\"\n",
    "tokenized_example = tokenizer(example_text)\n",
    "decoded_text = tokenizer.decode(tokenized_example[\"input_ids\"], skip_special_tokens=True)\n",
    "\n",
    "print(\"Tokenized:\", tokenized_example)\n",
    "print(\"Decoded:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T19:21:06.514680Z",
     "iopub.status.busy": "2025-01-14T19:21:06.514079Z",
     "iopub.status.idle": "2025-01-14T19:21:06.521522Z",
     "shell.execute_reply": "2025-01-14T19:21:06.520717Z",
     "shell.execute_reply.started": "2025-01-14T19:21:06.514653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ፈሪ', ' ተለዋዋጭነት', ' ሹል', ' ዓመታዊ', ' አጠቃላይ']\n"
     ]
    }
   ],
   "source": [
    "train_dataset=tokenized_dataset[\"train\"]\n",
    "print(train_dataset[\"word2\"][:5])  # Check if target texts are valid Amharic antonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:25:06.055709Z",
     "iopub.status.busy": "2025-01-26T15:25:06.055389Z",
     "iopub.status.idle": "2025-01-26T15:51:55.089126Z",
     "shell.execute_reply": "2025-01-26T15:51:55.088181Z",
     "shell.execute_reply.started": "2025-01-26T15:25:06.055683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d2e596ac4249428e5717878e108280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aed8cc434384b78bd7cbaf7b802a877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-10-c676684cbec1>:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomSeq2SeqTrainer(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1080/12000 25:47 < 4:21:11, 0.70 it/s, Epoch 9/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.301700</td>\n",
       "      <td>3.034365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.579400</td>\n",
       "      <td>0.544060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>0.386595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.369854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.294000</td>\n",
       "      <td>0.360923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.254800</td>\n",
       "      <td>0.358269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.240700</td>\n",
       "      <td>0.357911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.218000</td>\n",
       "      <td>0.361463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.360311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301aadb1319840cfa249c8e63d96ce61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1737905119.f9b8a834e73d.31.0:   0%|          | 0.00/31.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Beck90/m2m-amharic-antonym-augmented/commit/4ddc0e3b4dd2b27618f11a3dc68895fec1dd1530', commit_message='End of training', commit_description='', oid='4ddc0e3b4dd2b27618f11a3dc68895fec1dd1530', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Beck90/m2m-amharic-antonym-augmented', endpoint='https://huggingface.co', repo_type='model', repo_id='Beck90/m2m-amharic-antonym-augmented'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "# Load mT5 model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./m2m-amharic-antonym-augmented\",  # Directory to save the model\n",
    "    evaluation_strategy=\"epoch\",        # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,                 # Learning rate\n",
    "    per_device_train_batch_size=16,     # Training batch size\n",
    "    per_device_eval_batch_size=16,      # Evaluation batch size\n",
    "    num_train_epochs=100,                 # Number of epochs\n",
    "    weight_decay=0.01,                  # Weight decay for regularization\n",
    "    predict_with_generate=True,         # Allow prediction generation\n",
    "   # logging_dir=\"./logs\",               # Directory for logs\n",
    "    logging_steps=10,                   # Log every 10 steps\n",
    "    push_to_hub=True,                   # Enable pushing to Hugging Face hub\n",
    "    report_to=\"tensorboard\",\n",
    "    save_strategy= \"epoch\",  \n",
    "    save_total_limit=1,                 # Limit on the number of saved checkpoints\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "#from transformers import Seq2SeqTrainer\n",
    "\n",
    "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def _save(self, output_dir: str, state_dict=None):\n",
    "        if state_dict is None:\n",
    "            state_dict = self.model.state_dict()\n",
    "        \n",
    "        # Make all tensors in the state_dict contiguous\n",
    "        for key, tensor in state_dict.items():\n",
    "            if not tensor.is_contiguous():\n",
    "                state_dict[key] = tensor.contiguous()\n",
    "        \n",
    "        # Call the original save method\n",
    "        super()._save(output_dir, state_dict)\n",
    "\n",
    "trainer = CustomSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "# Push final model to Hugging Face Hub\n",
    "trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:53:16.858964Z",
     "iopub.status.busy": "2025-01-26T15:53:16.858681Z",
     "iopub.status.idle": "2025-01-26T15:53:20.621690Z",
     "shell.execute_reply": "2025-01-26T15:53:20.620941Z",
     "shell.execute_reply.started": "2025-01-26T15:53:16.858944Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ላይ\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./m2m-amharic-antonym-augmented\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate an antonym\n",
    "output = antonym_generator(\"ላይ\")\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:54:25.525677Z",
     "iopub.status.busy": "2025-01-26T15:54:25.525293Z",
     "iopub.status.idle": "2025-01-26T15:55:15.292179Z",
     "shell.execute_reply": "2025-01-26T15:55:15.291295Z",
     "shell.execute_reply.started": "2025-01-26T15:54:25.525649Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1258161a73b46438b91e8b86574151d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf71e090acd4e21b2477bfc8a0a39df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2d28b098434338b3c2b2f99e468002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ላይ\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-antonym-augmented\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline with GPU\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# Generate an antonym\n",
    "input_text = \"Antonym of: ላይ\"\n",
    "output = antonym_generator(input_text, max_length=16, num_return_sequences=1)\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:55:29.531062Z",
     "iopub.status.busy": "2025-01-26T15:55:29.530772Z",
     "iopub.status.idle": "2025-01-26T15:56:40.157510Z",
     "shell.execute_reply": "2025-01-26T15:56:40.156553Z",
     "shell.execute_reply.started": "2025-01-26T15:55:29.531042Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.26%\n",
      "BLEU Score: 0.37314820307067903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "test_dataset=tokenized_dataset[\"test\"]\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "for word in test_dataset[\"word1\"]:\n",
    "    # Tokenize the input word and move it to the same device as the model\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Generate antonym and decode\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=5)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Compute accuracy\n",
    "true_antonyms = test_dataset[\"word2\"]  # Ground truth antonyms\n",
    "accuracy = accuracy_score(true_antonyms, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute BLEU score (optional, for sequence evaluation)\n",
    "references = [[ref.split()] for ref in true_antonyms]\n",
    "predictions_tokenized = [pred.split() for pred in predictions]\n",
    "bleu = corpus_bleu(references, predictions_tokenized)\n",
    "print(f\"BLEU Score: {bleu}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:58:20.538284Z",
     "iopub.status.busy": "2025-01-26T15:58:20.537969Z",
     "iopub.status.idle": "2025-01-26T15:59:30.008916Z",
     "shell.execute_reply": "2025-01-26T15:59:30.007935Z",
     "shell.execute_reply.started": "2025-01-26T15:58:20.538260Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.78%\n",
      "BLEU Score: 0.39607242290087397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "test_dataset = tokenized_dataset[\"test\"]\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "for word in test_dataset[\"word1\"]:\n",
    "    # Add the prompt to the input\n",
    "    input_text = f\"Antonym of: {word}\"\n",
    "    \n",
    "    # Tokenize the input word and move it to the same device as the model\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Generate antonym and decode\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=5)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Compute accuracy\n",
    "true_antonyms = test_dataset[\"word2\"]  # Ground truth antonyms\n",
    "accuracy = accuracy_score(true_antonyms, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute BLEU score (optional, for sequence evaluation)\n",
    "references = [[ref.split()] for ref in true_antonyms]\n",
    "predictions_tokenized = [pred.split() for pred in predictions]\n",
    "bleu = corpus_bleu(references, predictions_tokenized)\n",
    "print(f\"BLEU Score: {bleu}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:55:46.176751Z",
     "iopub.status.busy": "2025-01-26T16:55:46.176414Z",
     "iopub.status.idle": "2025-01-26T16:56:37.259610Z",
     "shell.execute_reply": "2025-01-26T16:56:37.258628Z",
     "shell.execute_reply.started": "2025-01-26T16:55:46.176715Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3017022829254ebf885d4927fff74c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cb9461307d4793991955d088323649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b67b61338ef412c8dc61ff888c5fec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: በላ | Antonym: በላ\n",
      "Word: ብርድ | Antonym: ሙቀት\n",
      "Word: በረደ | Antonym: ነደደ\n",
      "Word: ሰጠ | Antonym: ጠፋ\n",
      "Word: አወራ | Antonym: ነጠላ\n",
      "Word: ሄደ | Antonym: ነደደ\n",
      "Word: ያዘ | Antonym: ያነሰ\n",
      "Word: መልካም | Antonym: አስቀያሚ\n",
      "Word: መጥፎ | Antonym: ደህና\n",
      "Word: አስቀያሚ | Antonym: አስቀያሚ\n",
      "Word: ፍላጎት | Antonym: ፍላጎት\n",
      "Word: ልባም | Antonym: እጥረት\n",
      "Word: ከፍ | Antonym: ዝቅ\n",
      "Word: ፍቅር | Antonym: ጠብቅ\n",
      "Word: ሙሉ | Antonym: ባዶ\n",
      "Word: ትልቅ | Antonym: ትንሽ\n",
      "Word: ቀላል | Antonym: አስቸጋሪ\n",
      "Word: ሰላም | Antonym: አለመግባባት\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-antonym-augmented\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# List of words for which to generate antonyms\n",
    "words = [\"በላ\", \"ብርድ\", \"በረደ\", \"ሰጠ\", \"አወራ\", \"ሄደ\", \"ያዘ\", \"መልካም\", \"መጥፎ\", \"አስቀያሚ\", \n",
    "         \"ፍላጎት\", \"ልባም\", \"ከፍ\", \"ፍቅር\", \"ሙሉ\", \"ትልቅ\", \"ቀላል\", \"ሰላም\"]\n",
    "\n",
    "# Generate antonyms for each word\n",
    "for word in words:\n",
    "    prompt = f\"Antonym of: {word}\"\n",
    "    output = antonym_generator(prompt)\n",
    "    print(f\"Word: {word} | Antonym: {output[0]['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T17:02:15.034915Z",
     "iopub.status.busy": "2025-01-26T17:02:15.034586Z",
     "iopub.status.idle": "2025-01-26T17:03:06.161507Z",
     "shell.execute_reply": "2025-01-26T17:03:06.160680Z",
     "shell.execute_reply.started": "2025-01-26T17:02:15.034890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcba24fdf314622b449666a4af09dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/931 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245d466f5c8f4f0198470b40323f7466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14518df3a504845ad95838d88a0a666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: በላ | Antonym: አሮጌ\n",
      "Word: ብርድ | Antonym: አለመግባባት\n",
      "Word: በረደ | Antonym: ያልተለመደ\n",
      "Word: ሰጠ | Antonym: ተከፈተ\n",
      "Word: አወራ | Antonym: ተጨማሪ\n",
      "Word: ሄደ | Antonym: ተከፈተ\n",
      "Word: ያዘ | Antonym: ያረጋጋል\n",
      "Word: መልካም | Antonym: ጨካኝ\n",
      "Word: መጥፎ | Antonym: ተስማሚ\n",
      "Word: አስቀያሚ | Antonym: አስደሳች\n",
      "Word: ፍላጎት | Antonym: ፍላጎት\n",
      "Word: ልባም | Antonym: ልማድ\n",
      "Word: ከፍ | Antonym: ዝቅተኛ\n",
      "Word: ፍቅር | Antonym: ጥላቻ\n",
      "Word: ሙሉ | Antonym: ቀጭን\n",
      "Word: ትልቅ | Antonym: ጥቂት\n",
      "Word: ቀላል | Antonym: ያልተለመደ\n",
      "Word: ሰላም | Antonym: አለመግባባት\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-antonym\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# List of words for which to generate antonyms\n",
    "words = [\"በላ\", \"ብርድ\", \"በረደ\", \"ሰጠ\", \"አወራ\", \"ሄደ\", \"ያዘ\", \"መልካም\", \"መጥፎ\", \"አስቀያሚ\", \n",
    "         \"ፍላጎት\", \"ልባም\", \"ከፍ\", \"ፍቅር\", \"ሙሉ\", \"ትልቅ\", \"ቀላል\", \"ሰላም\"]\n",
    "\n",
    "# Generate antonyms for each word\n",
    "for word in words:\n",
    "    prompt = f\"Antonym of: {word}\"\n",
    "    output = antonym_generator(prompt)\n",
    "    print(f\"Word: {word} | Antonym: {output[0]['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T17:06:22.178479Z",
     "iopub.status.busy": "2025-01-26T17:06:22.178151Z",
     "iopub.status.idle": "2025-01-26T17:07:12.883373Z",
     "shell.execute_reply": "2025-01-26T17:07:12.882523Z",
     "shell.execute_reply.started": "2025-01-26T17:06:22.178456Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534a372053da4cd193bfce84b977d1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/931 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa33fb2e8b3413ca7fa5cfa0732ec45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9006a1305b4e419384d7d82a49d1edff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: በላ | Antonym: አቋርጥ\n",
      "Word: ብርድ | Antonym: ደፋር\n",
      "Word: በረደ | Antonym: ያልተለመደ\n",
      "Word: ሰጠ | Antonym: ጠፋ\n",
      "Word: አወራ | Antonym: ፈሳሽ\n",
      "Word: ሄደ | Antonym: ተደሰተ\n",
      "Word: ያዘ | Antonym: ያረጋጋል\n",
      "Word: መልካም | Antonym: ጥንካሬ\n",
      "Word: መጥፎ | Antonym: ተጣብቋል\n",
      "Word: አስቀያሚ | Antonym: አስቀያሚ\n",
      "Word: ፍላጎት | Antonym: ፍላጎት\n",
      "Word: ልባም | Antonym: ልባም\n",
      "Word: ከፍ | Antonym: ከፍተኛ\n",
      "Word: ፍቅር | Antonym: ጥላቻ\n",
      "Word: ሙሉ | Antonym: በከፊል\n",
      "Word: ትልቅ | Antonym: ትልቅ\n",
      "Word: ቀላል | Antonym: ቀላል\n",
      "Word: ሰላም | Antonym: አለመግባባት\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-antonym2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# List of words for which to generate antonyms\n",
    "words = [\"በላ\", \"ብርድ\", \"በረደ\", \"ሰጠ\", \"አወራ\", \"ሄደ\", \"ያዘ\", \"መልካም\", \"መጥፎ\", \"አስቀያሚ\", \n",
    "         \"ፍላጎት\", \"ልባም\", \"ከፍ\", \"ፍቅር\", \"ሙሉ\", \"ትልቅ\", \"ቀላል\", \"ሰላም\"]\n",
    "\n",
    "# Generate antonyms for each word\n",
    "for word in words:\n",
    "    prompt = f\"Antonym of: {word}\"\n",
    "    output = antonym_generator(prompt)\n",
    "    print(f\"Word: {word} | Antonym: {output[0]['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T17:11:30.398383Z",
     "iopub.status.busy": "2025-01-26T17:11:30.398053Z",
     "iopub.status.idle": "2025-01-26T17:12:21.581591Z",
     "shell.execute_reply": "2025-01-26T17:12:21.580776Z",
     "shell.execute_reply.started": "2025-01-26T17:11:30.398358Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c193098331d4dd19f94c356af468a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680d4729e8304aa0931b945ac3aa1602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83733ddde54a4a7aa69d98e5f7f31c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: በላ | Antonym: በላ\n",
      "Word: ብርድ | Antonym: ብልህነት\n",
      "Word: በረደ | Antonym: የተበላሸ\n",
      "Word: ሰጠ | Antonym: ተንሳፋፊ\n",
      "Word: አወራ | Antonym: ቸልተኝነት\n",
      "Word: ሄደ | Antonym: ተንሳፋፊ\n",
      "Word: ያዘ | Antonym: ቸልተኝነት\n",
      "Word: መልካም | Antonym: እረፍት\n",
      "Word: መጥፎ | Antonym: የተዘበራረቀ\n",
      "Word: አስቀያሚ | Antonym: ቀናተኛ\n",
      "Word: ፍላጎት | Antonym: ፍላጎት\n",
      "Word: ልባም | Antonym: ብልህነት\n",
      "Word: ከፍ | Antonym: ከፍተኛ\n",
      "Word: ፍቅር | Antonym: እብድ\n",
      "Word: ሙሉ | Antonym: ሰፊ\n",
      "Word: ትልቅ | Antonym: ትልቅ\n",
      "Word: ቀላል | Antonym: ቀላል\n",
      "Word: ሰላም | Antonym: ይደሰቱ\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-synonym\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# List of words for which to generate antonyms\n",
    "words = [\"በላ\", \"ብርድ\", \"በረደ\", \"ሰጠ\", \"አወራ\", \"ሄደ\", \"ያዘ\", \"መልካም\", \"መጥፎ\", \"አስቀያሚ\", \n",
    "         \"ፍላጎት\", \"ልባም\", \"ከፍ\", \"ፍቅር\", \"ሙሉ\", \"ትልቅ\", \"ቀላል\", \"ሰላም\"]\n",
    "\n",
    "# Generate antonyms for each word\n",
    "for word in words:\n",
    "    prompt = f\"Synonym of: {word}\"\n",
    "    output = antonym_generator(prompt)\n",
    "    print(f\"Word: {word} | Synonym: {output[0]['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T17:12:43.366765Z",
     "iopub.status.busy": "2025-01-26T17:12:43.366443Z",
     "iopub.status.idle": "2025-01-26T17:13:34.201440Z",
     "shell.execute_reply": "2025-01-26T17:13:34.200578Z",
     "shell.execute_reply.started": "2025-01-26T17:12:43.366736Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbf046974254d16a0a14b463863f666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b97d295e4d346fa87210927e65a1076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ffc5725bde4d9c92c699da72250e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: በላ | Synonym: በላ\n",
      "Word: ብርድ | Synonym: ብልህነት\n",
      "Word: በረደ | Synonym: ተንሸራታች\n",
      "Word: ሰጠ | Synonym: ሰበሰበ\n",
      "Word: አወራ | Synonym: ተንሳፋፊ\n",
      "Word: ሄደ | Synonym: ተንሸራታች\n",
      "Word: ያዘ | Synonym: ተንሸራታች\n",
      "Word: መልካም | Synonym: እረፍት\n",
      "Word: መጥፎ | Synonym: የተዘበራረቀ\n",
      "Word: አስቀያሚ | Synonym: አስደንጋጭ\n",
      "Word: ፍላጎት | Synonym: ፍላጎት\n",
      "Word: ልባም | Synonym: ብልህነት\n",
      "Word: ከፍ | Synonym: ከፍተኛ\n",
      "Word: ፍቅር | Synonym: እብድ\n",
      "Word: ሙሉ | Synonym: ሰፊ\n",
      "Word: ትልቅ | Synonym: ትልቅ\n",
      "Word: ቀላል | Synonym: ቀላል\n",
      "Word: ሰላም | Synonym: ይደሰቱ\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-synonym2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# List of words for which to generate antonyms\n",
    "words = [\"በላ\", \"ብርድ\", \"በረደ\", \"ሰጠ\", \"አወራ\", \"ሄደ\", \"ያዘ\", \"መልካም\", \"መጥፎ\", \"አስቀያሚ\", \n",
    "         \"ፍላጎት\", \"ልባም\", \"ከፍ\", \"ፍቅር\", \"ሙሉ\", \"ትልቅ\", \"ቀላል\", \"ሰላም\"]\n",
    "\n",
    "# Generate antonyms for each word\n",
    "for word in words:\n",
    "    prompt = f\"Synonym of: {word}\"\n",
    "    output = antonym_generator(prompt)\n",
    "    print(f\"Word: {word} | Synonym: {output[0]['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:15:02.263341Z",
     "iopub.status.busy": "2025-01-26T16:15:02.263019Z",
     "iopub.status.idle": "2025-01-26T16:39:13.473179Z",
     "shell.execute_reply": "2025-01-26T16:39:13.472234Z",
     "shell.execute_reply.started": "2025-01-26T16:15:02.263308Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c006e9101f4f208c5caa8faa477906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e477755717647d0b1faf922d3269dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-8-c8db64dfb8c7>:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomSeq2SeqTrainer(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='960' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  960/12000 23:11 < 4:27:18, 0.69 it/s, Epoch 8/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.297700</td>\n",
       "      <td>3.032696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.574600</td>\n",
       "      <td>0.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.368600</td>\n",
       "      <td>0.384104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.297200</td>\n",
       "      <td>0.367217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.293200</td>\n",
       "      <td>0.359345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.251600</td>\n",
       "      <td>0.355499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.240100</td>\n",
       "      <td>0.356763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.211700</td>\n",
       "      <td>0.357865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9086a5f44af64b069748bf008fd6dc68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1737908115.60566b86f3b3.31.0:   0%|          | 0.00/28.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Beck90/m2m-amharic-antonym2-augmented/commit/d2bce0a11dc09281e37b52359683b92026857fad', commit_message='End of training', commit_description='', oid='d2bce0a11dc09281e37b52359683b92026857fad', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Beck90/m2m-amharic-antonym2-augmented', endpoint='https://huggingface.co', repo_type='model', repo_id='Beck90/m2m-amharic-antonym2-augmented'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "# Load mT5 model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./m2m-amharic-antonym2-augmented\",  # Directory to save the model\n",
    "    evaluation_strategy=\"epoch\",        # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,                 # Learning rate\n",
    "    per_device_train_batch_size=16,     # Training batch size\n",
    "    per_device_eval_batch_size=16,      # Evaluation batch size\n",
    "    num_train_epochs=100,                 # Number of epochs\n",
    "    weight_decay=0.01,                  # Weight decay for regularization\n",
    "    predict_with_generate=True,         # Allow prediction generation\n",
    "   # logging_dir=\"./logs\",               # Directory for logs\n",
    "    logging_steps=10,                   # Log every 10 steps\n",
    "    push_to_hub=True,                   # Enable pushing to Hugging Face hub\n",
    "    report_to=\"tensorboard\",\n",
    "    save_strategy= \"epoch\",  \n",
    "    save_total_limit=1,                 # Limit on the number of saved checkpoints\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "#from transformers import Seq2SeqTrainer\n",
    "\n",
    "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def custom_loss(output_logits, target_ids, embeddings, alpha=0.1):\n",
    "        # Reshape logits and target_ids\n",
    "        logits = output_logits.view(-1, output_logits.size(-1))  # Flatten logits\n",
    "        targets = target_ids.view(-1)                           # Flatten target IDs\n",
    "    \n",
    "        # Cross-Entropy Loss with ignored padding tokens\n",
    "        ce_loss = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)(logits, targets)\n",
    "    \n",
    "        # Cosine Similarity Loss (penalize dissimilar embeddings)\n",
    "        similarity_loss = 1 - F.cosine_similarity(embeddings[:, 0, :], embeddings[:, 1, :]).mean()\n",
    "    \n",
    "        # Combine losses\n",
    "        return ce_loss + alpha * similarity_loss\n",
    "\n",
    "\n",
    "\n",
    "trainer = CustomSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "# Push final model to Hugging Face Hub\n",
    "trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:41:09.337221Z",
     "iopub.status.busy": "2025-01-26T16:41:09.336886Z",
     "iopub.status.idle": "2025-01-26T16:41:13.734533Z",
     "shell.execute_reply": "2025-01-26T16:41:13.733774Z",
     "shell.execute_reply.started": "2025-01-26T16:41:09.337194Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ላይ\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./m2m-amharic-antonym2-augmented\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate an antonym\n",
    "output = antonym_generator(\"Antonym of: ላይ\")\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:46:24.840061Z",
     "iopub.status.busy": "2025-01-26T16:46:24.839723Z",
     "iopub.status.idle": "2025-01-26T16:46:24.962078Z",
     "shell.execute_reply": "2025-01-26T16:46:24.961205Z",
     "shell.execute_reply.started": "2025-01-26T16:46:24.840036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ሙቀት\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"./m2m-amharic-antonym2-augmented\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate an antonym\n",
    "output = antonym_generator(\"Antonym of: ብርድ\")\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:54:07.553650Z",
     "iopub.status.busy": "2025-01-26T16:54:07.553349Z",
     "iopub.status.idle": "2025-01-26T16:54:09.367535Z",
     "shell.execute_reply": "2025-01-26T16:54:09.366557Z",
     "shell.execute_reply.started": "2025-01-26T16:54:07.553626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: በላ | Antonym: በከለ\n",
      "Word: ብርድ | Antonym: ሙቀት\n",
      "Word: በረደ | Antonym: ነደደ\n",
      "Word: ሰጠ | Antonym: ነጠላ\n",
      "Word: አወራ | Antonym: ውሰዱ\n",
      "Word: ሄደ | Antonym: ተደሰተ\n",
      "Word: ያዘ | Antonym: ያነሰ\n",
      "Word: መልካም | Antonym: አስቀድሞ\n",
      "Word: መጥፎ | Antonym: ደህና\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: አስቀያሚ | Antonym: አስቀያሚ\n",
      "Word: ፍላጎት | Antonym: ጉጉት\n",
      "Word: ልባም | Antonym: ጠብቅ\n",
      "Word: ከፍ | Antonym: ዝቅ\n",
      "Word: ፍቅር | Antonym: ጠብቅ\n",
      "Word: ሙሉ | Antonym: ባዶ\n",
      "Word: ትልቅ | Antonym: ትንሽ\n",
      "Word: ቀላል | Antonym: ከባድ\n",
      "Word: ሰላም | Antonym: ብልሹነት\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"./m2m-amharic-antonym2-augmented\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Define a pipeline\n",
    "antonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# List of words for which to generate antonyms\n",
    "words = [\"በላ\", \"ብርድ\", \"በረደ\", \"ሰጠ\", \"አወራ\", \"ሄደ\", \"ያዘ\", \"መልካም\", \"መጥፎ\", \"አስቀያሚ\", \n",
    "         \"ፍላጎት\", \"ልባም\", \"ከፍ\", \"ፍቅር\", \"ሙሉ\", \"ትልቅ\", \"ቀላል\", \"ሰላም\"]\n",
    "\n",
    "# Generate antonyms for each word\n",
    "for word in words:\n",
    "    prompt = f\"Antonym of: {word}\"\n",
    "    output = antonym_generator(prompt)\n",
    "    print(f\"Word: {word} | Antonym: {output[0]['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:41:33.729205Z",
     "iopub.status.busy": "2025-01-26T16:41:33.728866Z",
     "iopub.status.idle": "2025-01-26T16:42:43.900453Z",
     "shell.execute_reply": "2025-01-26T16:42:43.899466Z",
     "shell.execute_reply.started": "2025-01-26T16:41:33.729175Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.15%\n",
      "BLEU Score: 0.3687150775982432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "test_dataset=tokenized_dataset[\"test\"]\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "for word in test_dataset[\"word1\"]:\n",
    "    # Tokenize the input word and move it to the same device as the model\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Generate antonym and decode\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=5)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Compute accuracy\n",
    "true_antonyms = test_dataset[\"word2\"]  # Ground truth antonyms\n",
    "accuracy = accuracy_score(true_antonyms, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute BLEU score (optional, for sequence evaluation)\n",
    "references = [[ref.split()] for ref in true_antonyms]\n",
    "predictions_tokenized = [pred.split() for pred in predictions]\n",
    "bleu = corpus_bleu(references, predictions_tokenized)\n",
    "print(f\"BLEU Score: {bleu}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T16:42:43.901721Z",
     "iopub.status.busy": "2025-01-26T16:42:43.901425Z",
     "iopub.status.idle": "2025-01-26T16:43:53.211040Z",
     "shell.execute_reply": "2025-01-26T16:43:53.210204Z",
     "shell.execute_reply.started": "2025-01-26T16:42:43.901676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.15%\n",
      "BLEU Score: 0.3656718058711154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "test_dataset = tokenized_dataset[\"test\"]\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "for word in test_dataset[\"word1\"]:\n",
    "    # Add the prompt to the input\n",
    "    input_text = f\"Antonym of: {word}\"\n",
    "    \n",
    "    # Tokenize the input word and move it to the same device as the model\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Generate antonym and decode\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=5)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Compute accuracy\n",
    "true_antonyms = test_dataset[\"word2\"]  # Ground truth antonyms\n",
    "accuracy = accuracy_score(true_antonyms, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute BLEU score (optional, for sequence evaluation)\n",
    "references = [[ref.split()] for ref in true_antonyms]\n",
    "predictions_tokenized = [pred.split() for pred in predictions]\n",
    "bleu = corpus_bleu(references, predictions_tokenized)\n",
    "print(f\"BLEU Score: {bleu}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6476400,
     "sourceId": 10461069,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6483494,
     "sourceId": 10471135,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
