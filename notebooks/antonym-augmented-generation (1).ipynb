{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10461069,"sourceType":"datasetVersion","datasetId":6476400},{"sourceId":10471135,"sourceType":"datasetVersion","datasetId":6483494}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ssh-keygen -t rsa -b 4096 -C bek.hailu@gmail.com","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T12:11:25.011841Z","iopub.execute_input":"2025-01-27T12:11:25.012122Z","iopub.status.idle":"2025-01-27T12:25:57.059759Z","shell.execute_reply.started":"2025-01-27T12:11:25.012102Z","shell.execute_reply":"2025-01-27T12:25:57.058979Z"}},"outputs":[{"name":"stdout","text":"Generating public/private rsa key pair.\nEnter file in which to save the key (/root/.ssh/id_rsa): ^C\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!ls https://www.kaggle.com/code/berekethailu/antonym-augmented-generation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T12:44:46.591257Z","iopub.execute_input":"2025-01-27T12:44:46.591724Z","iopub.status.idle":"2025-01-27T12:44:46.715209Z","shell.execute_reply.started":"2025-01-27T12:44:46.591690Z","shell.execute_reply":"2025-01-27T12:44:46.713546Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!git add berekethailu/antonym-augmented-generation.ipynb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T12:45:30.527556Z","iopub.execute_input":"2025-01-27T12:45:30.528066Z","iopub.status.idle":"2025-01-27T12:45:30.654644Z","shell.execute_reply.started":"2025-01-27T12:45:30.528029Z","shell.execute_reply":"2025-01-27T12:45:30.653272Z"}},"outputs":[{"name":"stdout","text":"fatal: pathspec 'berekethailu/antonym-augmented-generation.ipynb' did not match any files\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!~/.ssh/id_rsa.pub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git remote add origin git@github.com:Bekyy/Amharic-word-generation-GenAI-.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T12:39:15.176082Z","iopub.execute_input":"2025-01-27T12:39:15.176381Z","iopub.status.idle":"2025-01-27T12:39:15.294674Z","shell.execute_reply.started":"2025-01-27T12:39:15.176360Z","shell.execute_reply":"2025-01-27T12:39:15.293659Z"}},"outputs":[{"name":"stdout","text":"error: remote origin already exists.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!git clone https://github.com/Bekyy/Amharic-word-generation-GenAI-","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T12:25:59.330332Z","iopub.execute_input":"2025-01-27T12:25:59.330612Z","iopub.status.idle":"2025-01-27T12:26:00.110455Z","shell.execute_reply.started":"2025-01-27T12:25:59.330588Z","shell.execute_reply":"2025-01-27T12:26:00.109660Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Amharic-word-generation-GenAI-'...\nremote: Enumerating objects: 4, done.\u001b[K\nremote: Counting objects: 100% (4/4), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 4 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (4/4), 4.78 KiB | 1.59 MiB/s, done.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!git init\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T12:44:30.862913Z","iopub.execute_input":"2025-01-27T12:44:30.863334Z","iopub.status.idle":"2025-01-27T12:44:31.015828Z","shell.execute_reply.started":"2025-01-27T12:44:30.863302Z","shell.execute_reply":"2025-01-27T12:44:31.014193Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n\u001b[33mhint: \u001b[m\n\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n\u001b[33mhint: \u001b[m\n\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n\u001b[33mhint: \u001b[m\n\u001b[33mhint: \tgit branch -m <name>\u001b[m\nInitialized empty Git repository in /kaggle/working/.git/\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!cd Amharic-word-generation-GenAI- && ls\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T12:34:20.887150Z","iopub.execute_input":"2025-01-27T12:34:20.887462Z","iopub.status.idle":"2025-01-27T12:34:21.012378Z","shell.execute_reply.started":"2025-01-27T12:34:20.887437Z","shell.execute_reply":"2025-01-27T12:34:21.011524Z"}},"outputs":[{"name":"stdout","text":"LICENSE  README.md\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!git add https://www.kaggle.com/code/berekethailu/antonym-augmented-generation.ipynb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T12:35:26.037999Z","iopub.execute_input":"2025-01-27T12:35:26.038277Z","iopub.status.idle":"2025-01-27T12:35:26.157314Z","shell.execute_reply.started":"2025-01-27T12:35:26.038257Z","shell.execute_reply":"2025-01-27T12:35:26.156253Z"}},"outputs":[{"name":"stdout","text":"fatal: pathspec 'https://www.kaggle.com/code/berekethailu/antonym-augmented-generation.ipynb' did not match any files\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!git commit -m \"Add notebook from Kaggle\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T16:13:27.746857Z","iopub.execute_input":"2025-01-26T16:13:27.747086Z","iopub.status.idle":"2025-01-26T16:13:28.206314Z","shell.execute_reply.started":"2025-01-26T16:13:27.747065Z","shell.execute_reply":"2025-01-26T16:13:28.205425Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/antdataset/antonyms.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#!pip install transformers datasets torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the fdataset (ensure your CSV is UTF-8 encoded)\ndata = pd.read_csv('/kaggle/input/antdataset/antonyms.csv')\ndata = data[['word1','word2']]\ndata.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T16:13:28.207381Z","iopub.execute_input":"2025-01-26T16:13:28.207715Z","iopub.status.idle":"2025-01-26T16:13:28.243769Z","shell.execute_reply.started":"2025-01-26T16:13:28.207674Z","shell.execute_reply":"2025-01-26T16:13:28.242885Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"    word1   word2\n0      ·àã·ã≠      ·â≥·âΩ\n1      ·àã·ã≠  ·àã·àç·àò·àà·ä®·âµ\n2      ·àã·ã≠     ·àã·àã·ã≠\n3   ·çç·àã·åé·â∂·âΩ   ·å•·àã·âª·ãé·âΩ\n4     ·åç·âµ·à≠  ·ã®·àö·â∞·å£·å†·çç\n5   ·â∞·å†·äì·ä®·à®    ·â∞·ã≥·ä®·àò\n6   ·â∞·å†·äì·ä®·à®      ·àã·àã\n7  ·àô·àâ ·â†·àô·àâ    ·â†·ä®·çä·àç\n8  ·â∞·à∞·äì·ä≠·àè·àç    ·ä†·àç·çè·àç\n9  ·â∞·à∞·äì·ä≠·àè·àç  ·â∞·å†·äì·ä≠·àØ·àç","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word1</th>\n      <th>word2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>·àã·ã≠</td>\n      <td>·â≥·âΩ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>·àã·ã≠</td>\n      <td>·àã·àç·àò·àà·ä®·âµ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>·àã·ã≠</td>\n      <td>·àã·àã·ã≠</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>·çç·àã·åé·â∂·âΩ</td>\n      <td>·å•·àã·âª·ãé·âΩ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>·åç·âµ·à≠</td>\n      <td>·ã®·àö·â∞·å£·å†·çç</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>·â∞·å†·äì·ä®·à®</td>\n      <td>·â∞·ã≥·ä®·àò</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>·â∞·å†·äì·ä®·à®</td>\n      <td>·àã·àã</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>·àô·àâ ·â†·àô·àâ</td>\n      <td>·â†·ä®·çä·àç</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>·â∞·à∞·äì·ä≠·àè·àç</td>\n      <td>·ä†·àç·çè·àç</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>·â∞·à∞·äì·ä≠·àè·àç</td>\n      <td>·â∞·å†·äì·ä≠·àØ·àç</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\n# Load the original dataset\ndata = pd.read_csv('/kaggle/input/antdataset/antonyms.csv')\ndata = data[['word1', 'word2']]\n\n# Create a swapped DataFrame\nswapped_data = data.rename(columns={'word1': 'word2', 'word2': 'word1'})\n\n# Combine the original and swapped DataFrames\naugmented_data = pd.concat([data, swapped_data]).reset_index(drop=True)\n\n# Save the augmented data to a new CSV file\n#augmented_data.to_csv('/kaggle/working/augmented_antonyms.csv', index=False)\n\n# Display the augmented data\nprint(augmented_data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T16:14:20.026778Z","iopub.execute_input":"2025-01-26T16:14:20.027108Z","iopub.status.idle":"2025-01-26T16:14:20.043629Z","shell.execute_reply.started":"2025-01-26T16:14:20.027081Z","shell.execute_reply":"2025-01-26T16:14:20.043000Z"}},"outputs":[{"name":"stdout","text":"   word1   word2\n0     ·àã·ã≠      ·â≥·âΩ\n1     ·àã·ã≠  ·àã·àç·àò·àà·ä®·âµ\n2     ·àã·ã≠     ·àã·àã·ã≠\n3  ·çç·àã·åé·â∂·âΩ   ·å•·àã·âª·ãé·âΩ\n4    ·åç·âµ·à≠  ·ã®·àö·â∞·å£·å†·çç\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"augmented_data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T15:23:41.311825Z","iopub.execute_input":"2025-01-26T15:23:41.312109Z","iopub.status.idle":"2025-01-26T15:23:41.317466Z","shell.execute_reply.started":"2025-01-26T15:23:41.312090Z","shell.execute_reply":"2025-01-26T15:23:41.316782Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(4766, 2)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load your dataset\n# file_path = \"/kaggle/input/antdataset/antonyms.csv\"\n# data = pd.read_csv(file_path)\n# data = data[['word1','word2']]\n\n# Split data into 80% train and 20% test\ntrain_data, test_data = train_test_split(augmented_data, test_size=0.2, random_state=42)\n\n# Save the splits\ntrain_data.to_csv(\"train.csv\", index=False)\ntest_data.to_csv(\"test.csv\", index=False)\n\nprint(\"Train and test datasets saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T16:14:24.828869Z","iopub.execute_input":"2025-01-26T16:14:24.829152Z","iopub.status.idle":"2025-01-26T16:14:25.346432Z","shell.execute_reply.started":"2025-01-26T16:14:24.829131Z","shell.execute_reply":"2025-01-26T16:14:25.345599Z"}},"outputs":[{"name":"stdout","text":"Train and test datasets saved successfully!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"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\")  # Replace with your actual token\n# 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\n#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","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T16:14:28.370128Z","iopub.execute_input":"2025-01-26T16:14:28.370429Z","iopub.status.idle":"2025-01-26T16:14:29.033360Z","shell.execute_reply.started":"2025-01-26T16:14:28.370406Z","shell.execute_reply":"2025-01-26T16:14:29.032712Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#textdetox/mbart-detox-baseline\n#facebook/m2m100_418M\n#facebook/m2m100_1.2B\n#google/mt5-small","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T15:23:56.021913Z","iopub.execute_input":"2025-01-26T15:23:56.022196Z","iopub.status.idle":"2025-01-26T15:23:56.025524Z","shell.execute_reply.started":"2025-01-26T15:23:56.022174Z","shell.execute_reply":"2025-01-26T15:23:56.024716Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the CSV files as datasets\ndata_files = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\ndataset = load_dataset(\"csv\", data_files=data_files)\n\n# Check the loaded dataset\nprint(dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T16:14:33.447944Z","iopub.execute_input":"2025-01-26T16:14:33.448231Z","iopub.status.idle":"2025-01-26T16:14:34.521265Z","shell.execute_reply.started":"2025-01-26T16:14:33.448210Z","shell.execute_reply":"2025-01-26T16:14:34.520569Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5f0f71e5a7340d1a9a08c4385ef2d95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c1642a8c12f4dbb936cba62e1aada47"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['word1', 'word2'],\n        num_rows: 3812\n    })\n    test: Dataset({\n        features: ['word1', 'word2'],\n        num_rows: 954\n    })\n})\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n\n# Load mT5 tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\", )\n\n# Define tokenization function\ndef preprocess_function(examples):\n    inputs = [f\"Antonym of: {str(input_text)}\" for input_text in examples[\"word1\"]]\n    targets = [str(target_text) for target_text in examples[\"word2\"]]\n    \n    model_inputs = tokenizer(inputs, max_length=32, truncation=True, padding=\"max_length\")\n    labels = tokenizer(targets, max_length=32, truncation=True, padding=\"max_length\")\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Tokenize the dataset\ntokenized_dataset = dataset.map(preprocess_function, batched=True)\n\n# Check tokenized dataset\nprint(tokenized_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T16:14:38.826503Z","iopub.execute_input":"2025-01-26T16:14:38.826966Z","iopub.status.idle":"2025-01-26T16:15:02.261832Z","shell.execute_reply.started":"2025-01-26T16:14:38.826940Z","shell.execute_reply":"2025-01-26T16:15:02.261122Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"455f87104f854084b674d5244046be36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98771910266c4a738660a8a63592b514"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/3.71M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe517ae84d3b461392346e8545c629de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b02d0e23cbaa46dcb2cd9d7c7a21a5b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b038c3d4e924234a2d02b1a88a0a882"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3812 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28bba8494ba14187b6e411f46a636729"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/954 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f7d78a2d3774c44839fae075ae91881"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['word1', 'word2', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 3812\n    })\n    test: Dataset({\n        features: ['word1', 'word2', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 954\n    })\n})\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"example_text = \"·àã·ã≠\"\ntokenized_example = tokenizer(example_text)\nprint(tokenized_example)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:20:24.846232Z","iopub.execute_input":"2025-01-14T19:20:24.846515Z","iopub.status.idle":"2025-01-14T19:20:24.850955Z","shell.execute_reply.started":"2025-01-14T19:20:24.846494Z","shell.execute_reply":"2025-01-14T19:20:24.850176Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [128022, 3968, 2], 'attention_mask': [1, 1, 1]}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(tokenizer.convert_ids_to_tokens([128022, 3968, 2]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:20:48.534862Z","iopub.execute_input":"2025-01-14T19:20:48.535170Z","iopub.status.idle":"2025-01-14T19:20:48.539602Z","shell.execute_reply.started":"2025-01-14T19:20:48.535147Z","shell.execute_reply":"2025-01-14T19:20:48.538853Z"}},"outputs":[{"name":"stdout","text":"['__en__', '‚ñÅ·àã·ã≠', '</s>']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"example_text = \"·àã·ã≠ ·ä•·äì ·â≥·âΩ\"\ntokenized_example = tokenizer(example_text)\ndecoded_text = tokenizer.decode(tokenized_example[\"input_ids\"], skip_special_tokens=True)\n\nprint(\"Tokenized:\", tokenized_example)\nprint(\"Decoded:\", decoded_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:20:55.646149Z","iopub.execute_input":"2025-01-14T19:20:55.646452Z","iopub.status.idle":"2025-01-14T19:21:02.525783Z","shell.execute_reply.started":"2025-01-14T19:20:55.646429Z","shell.execute_reply":"2025-01-14T19:21:02.525032Z"}},"outputs":[{"name":"stdout","text":"Tokenized: {'input_ids': [128022, 3968, 3873, 11041, 5216, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}\nDecoded: ·àã·ã≠ ·ä•·äì ·â≥·âΩ\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"train_dataset=tokenized_dataset[\"train\"]\nprint(train_dataset[\"word2\"][:5])  # Check if target texts are valid Amharic antonyms\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:21:06.514079Z","iopub.execute_input":"2025-01-14T19:21:06.514680Z","iopub.status.idle":"2025-01-14T19:21:06.521522Z","shell.execute_reply.started":"2025-01-14T19:21:06.514653Z","shell.execute_reply":"2025-01-14T19:21:06.520717Z"}},"outputs":[{"name":"stdout","text":"['·çà·à™', ' ·â∞·àà·ãã·ãã·å≠·äê·âµ', ' ·àπ·àç', ' ·ãì·àò·â≥·ãä', ' ·ä†·å†·âÉ·àã·ã≠']\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n\n# Load mT5 model\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\")\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./m2m-amharic-antonym-augmented\",  # Directory to save the model\n    evaluation_strategy=\"epoch\",        # Evaluate at the end of each epoch\n    learning_rate=2e-5,                 # Learning rate\n    per_device_train_batch_size=16,     # Training batch size\n    per_device_eval_batch_size=16,      # Evaluation batch size\n    num_train_epochs=100,                 # Number of epochs\n    weight_decay=0.01,                  # Weight decay for regularization\n    predict_with_generate=True,         # Allow prediction generation\n   # logging_dir=\"./logs\",               # Directory for logs\n    logging_steps=10,                   # Log every 10 steps\n    push_to_hub=True,                   # Enable pushing to Hugging Face hub\n    report_to=\"tensorboard\",\n    save_strategy= \"epoch\",  \n    save_total_limit=1,                 # Limit on the number of saved checkpoints\n    load_best_model_at_end=True,\n)\n\nearly_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)\n\n#from transformers import Seq2SeqTrainer\n\nclass CustomSeq2SeqTrainer(Seq2SeqTrainer):\n    def _save(self, output_dir: str, state_dict=None):\n        if state_dict is None:\n            state_dict = self.model.state_dict()\n        \n        # Make all tensors in the state_dict contiguous\n        for key, tensor in state_dict.items():\n            if not tensor.is_contiguous():\n                state_dict[key] = tensor.contiguous()\n        \n        # Call the original save method\n        super()._save(output_dir, state_dict)\n\ntrainer = CustomSeq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    callbacks=[early_stopping_callback]\n)\n\n\ntrainer.train()\n# Push final model to Hugging Face Hub\ntrainer.push_to_hub()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T15:25:06.055389Z","iopub.execute_input":"2025-01-26T15:25:06.055709Z","iopub.status.idle":"2025-01-26T15:51:55.089126Z","shell.execute_reply.started":"2025-01-26T15:25:06.055683Z","shell.execute_reply":"2025-01-26T15:51:55.088181Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96d2e596ac4249428e5717878e108280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aed8cc434384b78bd7cbaf7b802a877"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-10-c676684cbec1>:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = CustomSeq2SeqTrainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1080' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1080/12000 25:47 < 4:21:11, 0.70 it/s, Epoch 9/100]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.301700</td>\n      <td>3.034365</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.579400</td>\n      <td>0.544060</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.367200</td>\n      <td>0.386595</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.299500</td>\n      <td>0.369854</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.294000</td>\n      <td>0.360923</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.254800</td>\n      <td>0.358269</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.240700</td>\n      <td>0.357911</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.218000</td>\n      <td>0.361463</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.186000</td>\n      <td>0.360311</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1737905119.f9b8a834e73d.31.0:   0%|          | 0.00/31.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"301aadb1319840cfa249c8e63d96ce61"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Beck90/m2m-amharic-antonym-augmented/commit/4ddc0e3b4dd2b27618f11a3dc68895fec1dd1530', commit_message='End of training', commit_description='', oid='4ddc0e3b4dd2b27618f11a3dc68895fec1dd1530', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Beck90/m2m-amharic-antonym-augmented', endpoint='https://huggingface.co', repo_type='model', repo_id='Beck90/m2m-amharic-antonym-augmented'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"./m2m-amharic-antonym-augmented\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n# Define a pipeline\nantonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n\n# Generate an antonym\noutput = antonym_generator(\"·àã·ã≠\")\nprint(output[0][\"generated_text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T15:53:16.858681Z","iopub.execute_input":"2025-01-26T15:53:16.858964Z","iopub.status.idle":"2025-01-26T15:53:20.621690Z","shell.execute_reply.started":"2025-01-26T15:53:16.858944Z","shell.execute_reply":"2025-01-26T15:53:20.620941Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"·àã·ã≠\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-antonym-augmented\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n# Define a pipeline with GPU\nantonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)\n\n# Generate an antonym\ninput_text = \"Antonym of: ·àã·ã≠\"\noutput = antonym_generator(input_text, max_length=16, num_return_sequences=1)\nprint(output[0][\"generated_text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T15:54:25.525293Z","iopub.execute_input":"2025-01-26T15:54:25.525677Z","iopub.status.idle":"2025-01-26T15:55:15.292179Z","shell.execute_reply.started":"2025-01-26T15:54:25.525649Z","shell.execute_reply":"2025-01-26T15:55:15.291295Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1258161a73b46438b91e8b86574151d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cf71e090acd4e21b2477bfc8a0a39df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f2d28b098434338b3c2b2f99e468002"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"·àã·ã≠\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import accuracy_score\nfrom nltk.translate.bleu_score import corpus_bleu\n\n# Ensure model is on the correct device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ntest_dataset=tokenized_dataset[\"test\"]\n# Generate predictions\npredictions = []\nfor word in test_dataset[\"word1\"]:\n    # Tokenize the input word and move it to the same device as the model\n    inputs = tokenizer(word, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # Generate antonym and decode\n    outputs = model.generate(inputs[\"input_ids\"], max_length=5)\n    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    predictions.append(prediction)\n\n# Compute accuracy\ntrue_antonyms = test_dataset[\"word2\"]  # Ground truth antonyms\naccuracy = accuracy_score(true_antonyms, predictions)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n\n# Compute BLEU score (optional, for sequence evaluation)\nreferences = [[ref.split()] for ref in true_antonyms]\npredictions_tokenized = [pred.split() for pred in predictions]\nbleu = corpus_bleu(references, predictions_tokenized)\nprint(f\"BLEU Score: {bleu}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T15:55:29.530772Z","iopub.execute_input":"2025-01-26T15:55:29.531062Z","iopub.status.idle":"2025-01-26T15:56:40.157510Z","shell.execute_reply.started":"2025-01-26T15:55:29.531042Z","shell.execute_reply":"2025-01-26T15:56:40.156553Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 1.26%\nBLEU Score: 0.37314820307067903\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import accuracy_score\nfrom nltk.translate.bleu_score import corpus_bleu\n\n# Ensure model is on the correct device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ntest_dataset = tokenized_dataset[\"test\"]\n# Generate predictions\npredictions = []\nfor word in test_dataset[\"word1\"]:\n    # Add the prompt to the input\n    input_text = f\"Antonym of: {word}\"\n    \n    # Tokenize the input word and move it to the same device as the model\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # Generate antonym and decode\n    outputs = model.generate(inputs[\"input_ids\"], max_length=5)\n    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    predictions.append(prediction)\n\n# Compute accuracy\ntrue_antonyms = test_dataset[\"word2\"]  # Ground truth antonyms\naccuracy = accuracy_score(true_antonyms, predictions)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n\n# Compute BLEU score (optional, for sequence evaluation)\nreferences = [[ref.split()] for ref in true_antonyms]\npredictions_tokenized = [pred.split() for pred in predictions]\nbleu = corpus_bleu(references, predictions_tokenized)\nprint(f\"BLEU Score: {bleu}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T15:58:20.537969Z","iopub.execute_input":"2025-01-26T15:58:20.538284Z","iopub.status.idle":"2025-01-26T15:59:30.008916Z","shell.execute_reply.started":"2025-01-26T15:58:20.538260Z","shell.execute_reply":"2025-01-26T15:59:30.007935Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 1.78%\nBLEU Score: 0.39607242290087397\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-antonym-augmented\")\n# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n# Define a pipeline\nantonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n\n# List of words for which to generate antonyms\nwords = [\"·â†·àã\", \"·â•·à≠·ãµ\", \"·â†·à®·ã∞\", \"·à∞·å†\", \"·ä†·ãà·à´\", \"·àÑ·ã∞\", \"·ã´·ãò\", \"·àò·àç·ä´·àù\", \"·àò·å•·çé\", \"·ä†·àµ·âÄ·ã´·àö\", \n         \"·çç·àã·åé·âµ\", \"·àç·â£·àù\", \"·ä®·çç\", \"·çç·âÖ·à≠\", \"·àô·àâ\", \"·âµ·àç·âÖ\", \"·âÄ·àã·àç\", \"·à∞·àã·àù\"]\n\n# Generate antonyms for each word\nfor word in words:\n    prompt = f\"Antonym of: {word}\"\n    output = antonym_generator(prompt)\n    print(f\"Word: {word} | Antonym: {output[0]['generated_text']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T16:55:46.176414Z","iopub.execute_input":"2025-01-26T16:55:46.176751Z","iopub.status.idle":"2025-01-26T16:56:37.259610Z","shell.execute_reply.started":"2025-01-26T16:55:46.176715Z","shell.execute_reply":"2025-01-26T16:56:37.258628Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3017022829254ebf885d4927fff74c34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02cb9461307d4793991955d088323649"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b67b61338ef412c8dc61ff888c5fec0"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Word: ·â†·àã | Antonym: ·â†·àã\nWord: ·â•·à≠·ãµ | Antonym: ·àô·âÄ·âµ\nWord: ·â†·à®·ã∞ | Antonym: ·äê·ã∞·ã∞\nWord: ·à∞·å† | Antonym: ·å†·çã\nWord: ·ä†·ãà·à´ | Antonym: ·äê·å†·àã\nWord: ·àÑ·ã∞ | Antonym: ·äê·ã∞·ã∞\nWord: ·ã´·ãò | Antonym: ·ã´·äê·à∞\nWord: ·àò·àç·ä´·àù | Antonym: ·ä†·àµ·âÄ·ã´·àö\nWord: ·àò·å•·çé | Antonym: ·ã∞·àÖ·äì\nWord: ·ä†·àµ·âÄ·ã´·àö | Antonym: ·ä†·àµ·âÄ·ã´·àö\nWord: ·çç·àã·åé·âµ | Antonym: ·çç·àã·åé·âµ\nWord: ·àç·â£·àù | Antonym: ·ä•·å•·à®·âµ\nWord: ·ä®·çç | Antonym: ·ãù·âÖ\nWord: ·çç·âÖ·à≠ | Antonym: ·å†·â•·âÖ\nWord: ·àô·àâ | Antonym: ·â£·ã∂\nWord: ·âµ·àç·âÖ | Antonym: ·âµ·äï·àΩ\nWord: ·âÄ·àã·àç | Antonym: ·ä†·àµ·â∏·åã·à™\nWord: ·à∞·àã·àù | Antonym: ·ä†·àà·àò·åç·â£·â£·âµ\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-antonym\")\n# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n# Define a pipeline\nantonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n\n# List of words for which to generate antonyms\nwords = [\"·â†·àã\", \"·â•·à≠·ãµ\", \"·â†·à®·ã∞\", \"·à∞·å†\", \"·ä†·ãà·à´\", \"·àÑ·ã∞\", \"·ã´·ãò\", \"·àò·àç·ä´·àù\", \"·àò·å•·çé\", \"·ä†·àµ·âÄ·ã´·àö\", \n         \"·çç·àã·åé·âµ\", \"·àç·â£·àù\", \"·ä®·çç\", \"·çç·âÖ·à≠\", \"·àô·àâ\", \"·âµ·àç·âÖ\", \"·âÄ·àã·àç\", \"·à∞·àã·àù\"]\n\n# Generate antonyms for each word\nfor word in words:\n    prompt = f\"Antonym of: {word}\"\n    output = antonym_generator(prompt)\n    print(f\"Word: {word} | Antonym: {output[0]['generated_text']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T17:02:15.034586Z","iopub.execute_input":"2025-01-26T17:02:15.034915Z","iopub.status.idle":"2025-01-26T17:03:06.161507Z","shell.execute_reply.started":"2025-01-26T17:02:15.034890Z","shell.execute_reply":"2025-01-26T17:03:06.160680Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/931 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bcba24fdf314622b449666a4af09dd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"245d466f5c8f4f0198470b40323f7466"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a14518df3a504845ad95838d88a0a666"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Word: ·â†·àã | Antonym: ·ä†·àÆ·åå\nWord: ·â•·à≠·ãµ | Antonym: ·ä†·àà·àò·åç·â£·â£·âµ\nWord: ·â†·à®·ã∞ | Antonym: ·ã´·àç·â∞·àà·àò·ã∞\nWord: ·à∞·å† | Antonym: ·â∞·ä®·çà·â∞\nWord: ·ä†·ãà·à´ | Antonym: ·â∞·å®·àõ·à™\nWord: ·àÑ·ã∞ | Antonym: ·â∞·ä®·çà·â∞\nWord: ·ã´·ãò | Antonym: ·ã´·à®·åã·åã·àç\nWord: ·àò·àç·ä´·àù | Antonym: ·å®·ä´·äù\nWord: ·àò·å•·çé | Antonym: ·â∞·àµ·àõ·àö\nWord: ·ä†·àµ·âÄ·ã´·àö | Antonym: ·ä†·àµ·ã∞·à≥·âΩ\nWord: ·çç·àã·åé·âµ | Antonym: ·çç·àã·åé·âµ\nWord: ·àç·â£·àù | Antonym: ·àç·àõ·ãµ\nWord: ·ä®·çç | Antonym: ·ãù·âÖ·â∞·äõ\nWord: ·çç·âÖ·à≠ | Antonym: ·å•·àã·âª\nWord: ·àô·àâ | Antonym: ·âÄ·å≠·äï\nWord: ·âµ·àç·âÖ | Antonym: ·å•·âÇ·âµ\nWord: ·âÄ·àã·àç | Antonym: ·ã´·àç·â∞·àà·àò·ã∞\nWord: ·à∞·àã·àù | Antonym: ·ä†·àà·àò·åç·â£·â£·âµ\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-antonym2\")\n# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n# Define a pipeline\nantonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n\n# List of words for which to generate antonyms\nwords = [\"·â†·àã\", \"·â•·à≠·ãµ\", \"·â†·à®·ã∞\", \"·à∞·å†\", \"·ä†·ãà·à´\", \"·àÑ·ã∞\", \"·ã´·ãò\", \"·àò·àç·ä´·àù\", \"·àò·å•·çé\", \"·ä†·àµ·âÄ·ã´·àö\", \n         \"·çç·àã·åé·âµ\", \"·àç·â£·àù\", \"·ä®·çç\", \"·çç·âÖ·à≠\", \"·àô·àâ\", \"·âµ·àç·âÖ\", \"·âÄ·àã·àç\", \"·à∞·àã·àù\"]\n\n# Generate antonyms for each word\nfor word in words:\n    prompt = f\"Antonym of: {word}\"\n    output = antonym_generator(prompt)\n    print(f\"Word: {word} | Antonym: {output[0]['generated_text']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T17:06:22.178151Z","iopub.execute_input":"2025-01-26T17:06:22.178479Z","iopub.status.idle":"2025-01-26T17:07:12.883373Z","shell.execute_reply.started":"2025-01-26T17:06:22.178456Z","shell.execute_reply":"2025-01-26T17:07:12.882523Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/931 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"534a372053da4cd193bfce84b977d1c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fa33fb2e8b3413ca7fa5cfa0732ec45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9006a1305b4e419384d7d82a49d1edff"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Word: ·â†·àã | Antonym: ·ä†·âã·à≠·å•\nWord: ·â•·à≠·ãµ | Antonym: ·ã∞·çã·à≠\nWord: ·â†·à®·ã∞ | Antonym: ·ã´·àç·â∞·àà·àò·ã∞\nWord: ·à∞·å† | Antonym: ·å†·çã\nWord: ·ä†·ãà·à´ | Antonym: ·çà·à≥·àΩ\nWord: ·àÑ·ã∞ | Antonym: ·â∞·ã∞·à∞·â∞\nWord: ·ã´·ãò | Antonym: ·ã´·à®·åã·åã·àç\nWord: ·àò·àç·ä´·àù | Antonym: ·å•·äï·ä´·à¨\nWord: ·àò·å•·çé | Antonym: ·â∞·å£·â•·âã·àç\nWord: ·ä†·àµ·âÄ·ã´·àö | Antonym: ·ä†·àµ·âÄ·ã´·àö\nWord: ·çç·àã·åé·âµ | Antonym: ·çç·àã·åé·âµ\nWord: ·àç·â£·àù | Antonym: ·àç·â£·àù\nWord: ·ä®·çç | Antonym: ·ä®·çç·â∞·äõ\nWord: ·çç·âÖ·à≠ | Antonym: ·å•·àã·âª\nWord: ·àô·àâ | Antonym: ·â†·ä®·çä·àç\nWord: ·âµ·àç·âÖ | Antonym: ·âµ·àç·âÖ\nWord: ·âÄ·àã·àç | Antonym: ·âÄ·àã·àç\nWord: ·à∞·àã·àù | Antonym: ·ä†·àà·àò·åç·â£·â£·âµ\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-synonym\")\n# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n# Define a pipeline\nantonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n\n# List of words for which to generate antonyms\nwords = [\"·â†·àã\", \"·â•·à≠·ãµ\", \"·â†·à®·ã∞\", \"·à∞·å†\", \"·ä†·ãà·à´\", \"·àÑ·ã∞\", \"·ã´·ãò\", \"·àò·àç·ä´·àù\", \"·àò·å•·çé\", \"·ä†·àµ·âÄ·ã´·àö\", \n         \"·çç·àã·åé·âµ\", \"·àç·â£·àù\", \"·ä®·çç\", \"·çç·âÖ·à≠\", \"·àô·àâ\", \"·âµ·àç·âÖ\", \"·âÄ·àã·àç\", \"·à∞·àã·àù\"]\n\n# Generate antonyms for each word\nfor word in words:\n    prompt = f\"Synonym of: {word}\"\n    output = antonym_generator(prompt)\n    print(f\"Word: {word} | Synonym: {output[0]['generated_text']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T17:11:30.398053Z","iopub.execute_input":"2025-01-26T17:11:30.398383Z","iopub.status.idle":"2025-01-26T17:12:21.581591Z","shell.execute_reply.started":"2025-01-26T17:11:30.398358Z","shell.execute_reply":"2025-01-26T17:12:21.580776Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c193098331d4dd19f94c356af468a2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"680d4729e8304aa0931b945ac3aa1602"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83733ddde54a4a7aa69d98e5f7f31c97"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Word: ·â†·àã | Antonym: ·â†·àã\nWord: ·â•·à≠·ãµ | Antonym: ·â•·àç·àÖ·äê·âµ\nWord: ·â†·à®·ã∞ | Antonym: ·ã®·â∞·â†·àã·à∏\nWord: ·à∞·å† | Antonym: ·â∞·äï·à≥·çã·çä\nWord: ·ä†·ãà·à´ | Antonym: ·â∏·àç·â∞·äù·äê·âµ\nWord: ·àÑ·ã∞ | Antonym: ·â∞·äï·à≥·çã·çä\nWord: ·ã´·ãò | Antonym: ·â∏·àç·â∞·äù·äê·âµ\nWord: ·àò·àç·ä´·àù | Antonym: ·ä•·à®·çç·âµ\nWord: ·àò·å•·çé | Antonym: ·ã®·â∞·ãò·â†·à´·à®·âÄ\nWord: ·ä†·àµ·âÄ·ã´·àö | Antonym: ·âÄ·äì·â∞·äõ\nWord: ·çç·àã·åé·âµ | Antonym: ·çç·àã·åé·âµ\nWord: ·àç·â£·àù | Antonym: ·â•·àç·àÖ·äê·âµ\nWord: ·ä®·çç | Antonym: ·ä®·çç·â∞·äõ\nWord: ·çç·âÖ·à≠ | Antonym: ·ä•·â•·ãµ\nWord: ·àô·àâ | Antonym: ·à∞·çä\nWord: ·âµ·àç·âÖ | Antonym: ·âµ·àç·âÖ\nWord: ·âÄ·àã·àç | Antonym: ·âÄ·àã·àç\nWord: ·à∞·àã·àù | Antonym: ·ã≠·ã∞·à∞·â±\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Beck90/m2m-amharic-synonym2\")\n# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n# Define a pipeline\nantonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n\n# List of words for which to generate antonyms\nwords = [\"·â†·àã\", \"·â•·à≠·ãµ\", \"·â†·à®·ã∞\", \"·à∞·å†\", \"·ä†·ãà·à´\", \"·àÑ·ã∞\", \"·ã´·ãò\", \"·àò·àç·ä´·àù\", \"·àò·å•·çé\", \"·ä†·àµ·âÄ·ã´·àö\", \n         \"·çç·àã·åé·âµ\", \"·àç·â£·àù\", \"·ä®·çç\", \"·çç·âÖ·à≠\", \"·àô·àâ\", \"·âµ·àç·âÖ\", \"·âÄ·àã·àç\", \"·à∞·àã·àù\"]\n\n# Generate antonyms for each word\nfor word in words:\n    prompt = f\"Synonym of: {word}\"\n    output = antonym_generator(prompt)\n    print(f\"Word: {word} | Synonym: {output[0]['generated_text']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T17:12:43.366443Z","iopub.execute_input":"2025-01-26T17:12:43.366765Z","iopub.status.idle":"2025-01-26T17:13:34.201440Z","shell.execute_reply.started":"2025-01-26T17:12:43.366736Z","shell.execute_reply":"2025-01-26T17:13:34.200578Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dbf046974254d16a0a14b463863f666"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b97d295e4d346fa87210927e65a1076"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22ffc5725bde4d9c92c699da72250e89"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Word: ·â†·àã | Synonym: ·â†·àã\nWord: ·â•·à≠·ãµ | Synonym: ·â•·àç·àÖ·äê·âµ\nWord: ·â†·à®·ã∞ | Synonym: ·â∞·äï·à∏·à´·â≥·âΩ\nWord: ·à∞·å† | Synonym: ·à∞·â†·à∞·â†\nWord: ·ä†·ãà·à´ | Synonym: ·â∞·äï·à≥·çã·çä\nWord: ·àÑ·ã∞ | Synonym: ·â∞·äï·à∏·à´·â≥·âΩ\nWord: ·ã´·ãò | Synonym: ·â∞·äï·à∏·à´·â≥·âΩ\nWord: ·àò·àç·ä´·àù | Synonym: ·ä•·à®·çç·âµ\nWord: ·àò·å•·çé | Synonym: ·ã®·â∞·ãò·â†·à´·à®·âÄ\nWord: ·ä†·àµ·âÄ·ã´·àö | Synonym: ·ä†·àµ·ã∞·äï·åã·å≠\nWord: ·çç·àã·åé·âµ | Synonym: ·çç·àã·åé·âµ\nWord: ·àç·â£·àù | Synonym: ·â•·àç·àÖ·äê·âµ\nWord: ·ä®·çç | Synonym: ·ä®·çç·â∞·äõ\nWord: ·çç·âÖ·à≠ | Synonym: ·ä•·â•·ãµ\nWord: ·àô·àâ | Synonym: ·à∞·çä\nWord: ·âµ·àç·âÖ | Synonym: ·âµ·àç·âÖ\nWord: ·âÄ·àã·àç | Synonym: ·âÄ·àã·àç\nWord: ·à∞·àã·àù | Synonym: ·ã≠·ã∞·à∞·â±\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## With custom loss","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\nfrom torch.nn import CrossEntropyLoss\nimport torch.nn.functional as F\n# Load mT5 model\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\")\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./m2m-amharic-antonym2-augmented\",  # Directory to save the model\n    evaluation_strategy=\"epoch\",        # Evaluate at the end of each epoch\n    learning_rate=2e-5,                 # Learning rate\n    per_device_train_batch_size=16,     # Training batch size\n    per_device_eval_batch_size=16,      # Evaluation batch size\n    num_train_epochs=100,                 # Number of epochs\n    weight_decay=0.01,                  # Weight decay for regularization\n    predict_with_generate=True,         # Allow prediction generation\n   # logging_dir=\"./logs\",               # Directory for logs\n    logging_steps=10,                   # Log every 10 steps\n    push_to_hub=True,                   # Enable pushing to Hugging Face hub\n    report_to=\"tensorboard\",\n    save_strategy= \"epoch\",  \n    save_total_limit=1,                 # Limit on the number of saved checkpoints\n    load_best_model_at_end=True,\n)\n\nearly_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)\n\n#from transformers import Seq2SeqTrainer\n\nclass CustomSeq2SeqTrainer(Seq2SeqTrainer):\n    def custom_loss(output_logits, target_ids, embeddings, alpha=0.1):\n        # Reshape logits and target_ids\n        logits = output_logits.view(-1, output_logits.size(-1))  # Flatten logits\n        targets = target_ids.view(-1)                           # Flatten target IDs\n    \n        # Cross-Entropy Loss with ignored padding tokens\n        ce_loss = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)(logits, targets)\n    \n        # Cosine Similarity Loss (penalize dissimilar embeddings)\n        similarity_loss = 1 - F.cosine_similarity(embeddings[:, 0, :], embeddings[:, 1, :]).mean()\n    \n        # Combine losses\n        return ce_loss + alpha * similarity_loss\n\n\n\ntrainer = CustomSeq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    callbacks=[early_stopping_callback]\n)\n\n\ntrainer.train()\n# Push final model to Hugging Face Hub\ntrainer.push_to_hub()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T16:15:02.263019Z","iopub.execute_input":"2025-01-26T16:15:02.263341Z","iopub.status.idle":"2025-01-26T16:39:13.473179Z","shell.execute_reply.started":"2025-01-26T16:15:02.263308Z","shell.execute_reply":"2025-01-26T16:39:13.472234Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71c006e9101f4f208c5caa8faa477906"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e477755717647d0b1faf922d3269dfb"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-8-c8db64dfb8c7>:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = CustomSeq2SeqTrainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='960' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  960/12000 23:11 < 4:27:18, 0.69 it/s, Epoch 8/100]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.297700</td>\n      <td>3.032696</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.574600</td>\n      <td>0.543000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.368600</td>\n      <td>0.384104</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.297200</td>\n      <td>0.367217</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.293200</td>\n      <td>0.359345</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.251600</td>\n      <td>0.355499</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.240100</td>\n      <td>0.356763</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.211700</td>\n      <td>0.357865</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1737908115.60566b86f3b3.31.0:   0%|          | 0.00/28.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9086a5f44af64b069748bf008fd6dc68"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Beck90/m2m-amharic-antonym2-augmented/commit/d2bce0a11dc09281e37b52359683b92026857fad', commit_message='End of training', commit_description='', oid='d2bce0a11dc09281e37b52359683b92026857fad', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Beck90/m2m-amharic-antonym2-augmented', endpoint='https://huggingface.co', repo_type='model', repo_id='Beck90/m2m-amharic-antonym2-augmented'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"./m2m-amharic-antonym2-augmented\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n# Define a pipeline\nantonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n\n# Generate an antonym\noutput = antonym_generator(\"Antonym of: ·àã·ã≠\")\nprint(output[0][\"generated_text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T16:41:09.336886Z","iopub.execute_input":"2025-01-26T16:41:09.337221Z","iopub.status.idle":"2025-01-26T16:41:13.734533Z","shell.execute_reply.started":"2025-01-26T16:41:09.337194Z","shell.execute_reply":"2025-01-26T16:41:13.733774Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"·àã·ã≠\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the fine-tuned model and tokenizer\n# model = AutoModelForSeq2SeqLM.from_pretrained(\"./m2m-amharic-antonym2-augmented\")\n# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n# Define a pipeline\nantonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n\n# Generate an antonym\noutput = antonym_generator(\"Antonym of: ·â•·à≠·ãµ\")\nprint(output[0][\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T16:46:24.839723Z","iopub.execute_input":"2025-01-26T16:46:24.840061Z","iopub.status.idle":"2025-01-26T16:46:24.962078Z","shell.execute_reply.started":"2025-01-26T16:46:24.840036Z","shell.execute_reply":"2025-01-26T16:46:24.961205Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"·àô·âÄ·âµ\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the fine-tuned model and tokenizer\n# model = AutoModelForSeq2SeqLM.from_pretrained(\"./m2m-amharic-antonym2-augmented\")\n# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n# Define a pipeline\nantonym_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n\n# List of words for which to generate antonyms\nwords = [\"·â†·àã\", \"·â•·à≠·ãµ\", \"·â†·à®·ã∞\", \"·à∞·å†\", \"·ä†·ãà·à´\", \"·àÑ·ã∞\", \"·ã´·ãò\", \"·àò·àç·ä´·àù\", \"·àò·å•·çé\", \"·ä†·àµ·âÄ·ã´·àö\", \n         \"·çç·àã·åé·âµ\", \"·àç·â£·àù\", \"·ä®·çç\", \"·çç·âÖ·à≠\", \"·àô·àâ\", \"·âµ·àç·âÖ\", \"·âÄ·àã·àç\", \"·à∞·àã·àù\"]\n\n# Generate antonyms for each word\nfor word in words:\n    prompt = f\"Antonym of: {word}\"\n    output = antonym_generator(prompt)\n    print(f\"Word: {word} | Antonym: {output[0]['generated_text']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T16:54:07.553349Z","iopub.execute_input":"2025-01-26T16:54:07.553650Z","iopub.status.idle":"2025-01-26T16:54:09.367535Z","shell.execute_reply.started":"2025-01-26T16:54:07.553626Z","shell.execute_reply":"2025-01-26T16:54:09.366557Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Word: ·â†·àã | Antonym: ·â†·ä®·àà\nWord: ·â•·à≠·ãµ | Antonym: ·àô·âÄ·âµ\nWord: ·â†·à®·ã∞ | Antonym: ·äê·ã∞·ã∞\nWord: ·à∞·å† | Antonym: ·äê·å†·àã\nWord: ·ä†·ãà·à´ | Antonym: ·ãç·à∞·ã±\nWord: ·àÑ·ã∞ | Antonym: ·â∞·ã∞·à∞·â∞\nWord: ·ã´·ãò | Antonym: ·ã´·äê·à∞\nWord: ·àò·àç·ä´·àù | Antonym: ·ä†·àµ·âÄ·ãµ·àû\nWord: ·àò·å•·çé | Antonym: ·ã∞·àÖ·äì\n","output_type":"stream"},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"Word: ·ä†·àµ·âÄ·ã´·àö | Antonym: ·ä†·àµ·âÄ·ã´·àö\nWord: ·çç·àã·åé·âµ | Antonym: ·åâ·åâ·âµ\nWord: ·àç·â£·àù | Antonym: ·å†·â•·âÖ\nWord: ·ä®·çç | Antonym: ·ãù·âÖ\nWord: ·çç·âÖ·à≠ | Antonym: ·å†·â•·âÖ\nWord: ·àô·àâ | Antonym: ·â£·ã∂\nWord: ·âµ·àç·âÖ | Antonym: ·âµ·äï·àΩ\nWord: ·âÄ·àã·àç | Antonym: ·ä®·â£·ãµ\nWord: ·à∞·àã·àù | Antonym: ·â•·àç·àπ·äê·âµ\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import accuracy_score\nfrom nltk.translate.bleu_score import corpus_bleu\n\n# Ensure model is on the correct device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ntest_dataset=tokenized_dataset[\"test\"]\n# Generate predictions\npredictions = []\nfor word in test_dataset[\"word1\"]:\n    # Tokenize the input word and move it to the same device as the model\n    inputs = tokenizer(word, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # Generate antonym and decode\n    outputs = model.generate(inputs[\"input_ids\"], max_length=5)\n    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    predictions.append(prediction)\n\n# Compute accuracy\ntrue_antonyms = test_dataset[\"word2\"]  # Ground truth antonyms\naccuracy = accuracy_score(true_antonyms, predictions)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n\n# Compute BLEU score (optional, for sequence evaluation)\nreferences = [[ref.split()] for ref in true_antonyms]\npredictions_tokenized = [pred.split() for pred in predictions]\nbleu = corpus_bleu(references, predictions_tokenized)\nprint(f\"BLEU Score: {bleu}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T16:41:33.728866Z","iopub.execute_input":"2025-01-26T16:41:33.729205Z","iopub.status.idle":"2025-01-26T16:42:43.900453Z","shell.execute_reply.started":"2025-01-26T16:41:33.729175Z","shell.execute_reply":"2025-01-26T16:42:43.899466Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 1.15%\nBLEU Score: 0.3687150775982432\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import accuracy_score\nfrom nltk.translate.bleu_score import corpus_bleu\n\n# Ensure model is on the correct device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ntest_dataset = tokenized_dataset[\"test\"]\n# Generate predictions\npredictions = []\nfor word in test_dataset[\"word1\"]:\n    # Add the prompt to the input\n    input_text = f\"Antonym of: {word}\"\n    \n    # Tokenize the input word and move it to the same device as the model\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # Generate antonym and decode\n    outputs = model.generate(inputs[\"input_ids\"], max_length=5)\n    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    predictions.append(prediction)\n\n# Compute accuracy\ntrue_antonyms = test_dataset[\"word2\"]  # Ground truth antonyms\naccuracy = accuracy_score(true_antonyms, predictions)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n\n# Compute BLEU score (optional, for sequence evaluation)\nreferences = [[ref.split()] for ref in true_antonyms]\npredictions_tokenized = [pred.split() for pred in predictions]\nbleu = corpus_bleu(references, predictions_tokenized)\nprint(f\"BLEU Score: {bleu}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T16:42:43.901425Z","iopub.execute_input":"2025-01-26T16:42:43.901721Z","iopub.status.idle":"2025-01-26T16:43:53.211040Z","shell.execute_reply.started":"2025-01-26T16:42:43.901676Z","shell.execute_reply":"2025-01-26T16:43:53.210204Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 1.15%\nBLEU Score: 0.3656718058711154\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}